---
title: "Seminar 6: Naive Bayes Classifier"
layout: post
use_math: true
tags: ["seminar"]
---

<br/>

여러분이 [HW5]({{"/2021/12/21/hw-5.html" | relative_url}})의 과제를 열심히 했다면, 자연어 데이터를 전처리하는 방법 중 하나인 "토큰화(tokenization)"을 잘 이해했을 것이다 👏

이번 포스트에서는 실제 데이터에서 토큰화를 적용하고, 간단한 ML 모델을 만들어보겠다.

<hr/>

## Spam Classifier

[SNS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)에는 5천개 정도의 이메일 데이터가 있다. 


```text
     v1                                                 v2
0   ham  Go until jurong point, crazy.. Available only ...
1   ham                      Ok lar... Joking wif u oni...
2  spam  Free entry in 2 a wkly comp to win FA Cup fina...
3   ham  U dun say so early hor... U c already then say...
4   ham  Nah I don't think he goes to usf, he lives aro...
```

메일 본문과 함께 스팸(spam) 메일과 일반(ham) 메일로 레이블이 있는데, 이 데이터으로 스팸인지 아닌지 분류하는 간단한 분류기를 구현해보겠다.

## Naive Bayes Classifier

데이터를 서로 다른 레이블로 분류하는 문제를 푸는 classifier에는 여러가지 종류가 있다. 

- Logistic Regression
- KNN Classifier
- Naive Bayes Classifier
- Decision-tree based classifiers...

이번 포스트에서는 그 중에서 \<Naive Bayes classifier\>라는 분류기를 직접 구현해볼 것이다.

### 이론

먼저 Naive Bayes의 이론을 하나씩 살펴보자.

NB의 목표는 $p(c_k \mid x)$의 조건부 확률을 구하는 것이다. 이 확률은 데이터 $x$가 주어졌을 때 레이블 $c_k$일 확률을 말한다. 좀더 풀어쓰면...

- $p(S = T \mid x)$: 데이터 $x$가 스팸(spam)일 확률
- $p(S = F \mid x)$: 데이터 $x$가 햄(ham)일 확률

그런데 위와 같은 조건부 확률은 베이즈 정리(Bayes Theorem)에 의해 아래와 같이 풀어 쓸 수 있다.

$$
p(c_k \mid x) = \frac{p(c_k) \cdot p(x \mid c_k)}{p(x)}
$$

분모 형태에서 각 확률의 의미를 살펴보면

- $p(c_k)$: 레이블이 $c_k$일 확률
  - $p(S=T)$: 스팸 메일일 확률 = (스팸 메일의 수) / (전체 메일의 수)
- $p(x \mid c_k)$: 레이블이 $c_k$일 때, 데이터 $x$가 등장할 확률
- $p(x)$: 데이터 $x$가 등장할 확률

이때 $p(x)$는 생각할 필요가 없는게, 정확한 값을 구할 수 없기 때문이다. 우리가 지금 보는 스팸 메일 분류 문제라면, 데이터 $x$는 `"Go until jurong point, crazy.."`라는 메일 본문에 해당하는데, 이 메일 본문은 아주 거대한 코퍼스 공간 위의 한 점에 불과하기 때문에 확률이 정말 낮다. 

물론 확률을 정의하는 전체 공간을 명확히 정의하면 실제 $p(x)$의 값을 구할 수도 있을 것이다. 그러나 지금 우리가 다루는 문제에서는 $p(x)$ 값을 아는 것이 전혀 의미가 없다.

일단 우리는 $p(c_k \mid x)$의 값이 큰 레이블을 정답으로 여길 것이다. 그래서

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k \mid x)
$$

을 구하게 되는데, $p(S =T \mid x)$에서나 $p(S = F \mid x)$에서나 $p(x)$는 공통되는 상수이기 때문에 output $y$를 구하는데 전혀 영향을 주지 않는다. 그래서

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(x \mid c_k)
$$

여기까지가 NB classifier에서 'Bayes'라는 이름이 붙는 이유다!

<br/>

다음 단계는 $p(x \mid c_k)$를 raw한 데이터 $x$ 그대로 쓰는게 아니라 feature $(a_1, ..., a_n)$의 형태로 기술하는 것이다. 우리는 메일 본문에서 몇가지 특징적인 부분들을 정할 수 있다. `'free'`라는 단어의 유무, `'Call'`라는 단어의 유무, 본문의 길이, 같은 단어가 최대 몇번 반복되는지 등등... Feature Extraction을 수행하면 raw text가 아니라 정량적인 feature의 형태로 데이터를 표현할 수 있다.

NB의 경우는 특정 단어가 등장 하는지 여부를 feature로 사용한다. 그래서 $p(w = \text{free} \mid S = T)$라고 하면... 스팸인 메일 중에서 $\text{free}$라는 단어가 등장할 확률을 말한다! 이것은 

$$
p(w = \text{free} \mid S = T) = \frac{\text{#. of spam mails contain word 'free'}}{\text{#. of spam mails}}
$$

로 쉽게 확률을 구할 수 있다! 👏

<br/>

이제 $p(x \mid c_k)$를 feature $(a_1, ..., a_n)$의 형태로 기술하면...

$$
p(x \mid c_k) = p(a_1, ..., a_n \mid c_k)
$$

그런데 NB는 각 피처 사이에 \<naive assumption\>이라는 가정을 한다. 이것은 아래와 같다.

<div class="statement" markdown="1" align="center">

"One assumption taken is the <span style="color:red;">strong independence</span> assumptions btw the features."

</div>

즉, 각 피쳐가 서로 **독립**이다!를 가정한다. 이것은 곧 $p(a_1, ..., a_n \mid c_k)$에 대해 아래가 성립함을 말한다.

$$
p(a_1, ..., a_n \mid c_k)
= p(a_1 \mid c_k) p(a_2 \mid c_k) \cdots p(a_n \mid c_k)
$$

그래서 각 피처의 확률을 단순히 곱하는 것만으로 joint probability $p(a_1, ..., a_n \mid c_k)$를 구할 수 있다! 👍

물론 strong indepence 가정은 모델링을 위해 적당한 형태로 가정한 것일 뿐이다. 실제론 각 피처가 독립이 아니라 correlate 되어 있을 수 있다.

<br/>

자! 그럼 원래 구하려고 했던 output $y$를 다시 구해보자.

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(x \mid c_k)
$$

$p(x \mid c_k)$가 feature $a_i$로 바뀌고 \<naive assumption\>에 의해 위의 식은 아래가 된다.

$$
\begin{aligned}
y 
&= \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(a_1 \mid c_k) \cdots p(a_n \mid c_k) \\
&= \underset{c_k}{\text{argmax}} \; p(c_k) \cdot \prod_i^n p(a_i \mid c_k)
\end{aligned}
$$

결국 각 레이블 $c_k$에 대해, $S=T$, $S=F$에 대해 $p(c_k) \cdot \prod_i^n p(a_i \mid c_k)$ 값을 구해 두 값을 비교해 더 큰 레이블의 값을 output $y$로 매기면 되는 것이다! 🙌

<hr/>

### 구현

자... 그럼 이걸 코드로 구현해보자! 사실 구현 자체도 그리 어렵지 않으니 잘 따라와보자 😉

일단 스팸 데이터셋 [SNS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)을 준비한다. 

```bash
!wget https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv
```

그리고 적당히 데이터를 가공하고, train-test split을 수행하면...

```py
data = pd.read_csv('spam.csv', encoding='latin1')
print("샘플 수", len(data)) # 5,572
data.head()

data = data[['v1', 'v2']] # 필요한 컬럼한 추출
data.head()

data['v1'] = data['v1'].replace(['ham','spam'],[0,1]) # label을 0, 1로 변경
data.head()
```

```py
data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)

X_train = data_train['v2']
y_train = data_train['v1']
X_test = data_test['v2']
y_test = data_test['v1']

print('=== 훈련 데이터 비율 ===')
print('ham', str(round(np.sum(y_train == 0) / len(y_train) * 100, 1)) + '%')
print('spam', str(round(np.sum(y_train == 1) / len(y_train) * 100, 1)) + '%')
print('=== 테스트 데이터 비율 ===')
print('ham', str(round(np.sum(y_test == 0) / len(y_test) * 100, 1)) + '%')
print('spam', str(round(np.sum(y_test == 1) / len(y_test) * 100, 1)) + '%')

...

=== 훈련 데이터 비율 ===
ham 86.4%
spam 13.6%
=== 테스트 데이터 비율 ===
ham 87.5%
spam 12.5%
```

<br/>

메일 본문 자체는 너무 raw한 데이터이기 때문에 바로 쓰기 보다는 전처리를 해주는게 좋다. [HW5]({{"/2021/12/21/hw-5.html" | relative_url}}) 과제에서 배운 토큰화(tokenization)을 활용해보자!

```py
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
```

``` py
def tokenize(text):
  sentences = sent_tokenize(text)
  tokens = []

  for sentence in sentences:
    # 단어 토큰화
    words = word_tokenize(sentence)
    for word in words:
      word = word.lower() # 소문자 변환
      if word in stop_words: continue # 불용어 제거
      if len(word) <= 3: continue # 길이 3 이하 제거
      tokens.append(word)
  return tokens
```

텍스트에서 문장 토큰화와 단어 토큰화를 한 후, 불용어를 제거하고 길이 3 이하의 단어들을 모두 제거했다!

그리고 `vocab = {}`에 각 토큰의 빈도수를 저장하면...

```py
vocab = {}
for text in X_train:
  # 토큰화
  tokens = tokenize(text)

  for token in tokens:
    if token not in vocab:
      vocab[token] = 0
    vocab[token] += 1

print(vocab)
print(len(vocab))

...

{'sleeping': 10, 'feeling': 15, 'well': 86, 'come': 177, ...
7449
```

와 같이 출력 결과를 얻을 수 있다. 그런데 이 모든 토큰을 다 쓸 건 아니고 빈도수 Top 5의 단어에 대해서만 feature extraction을 적용할 것이다. 그래서 적당히 정렬해 `target_token`을 정하면...

```py
# filter out top 5 tokens
vocab_sorted = sorted(vocab.items(), key=lambda x : x[1], reverse=True)

vocab_size = 5
vocab_sorted = vocab_sorted[:vocab_size]
print(vocab_sorted)
target_tokens = [word for word, _ in vocab_sorted]
print(target_tokens)

...

[('call', 460), ('know', 216), ('free', 212), ('like', 204), ('good', 193)]
['call', 'know', 'free', 'like', 'good']
```

위와 같이 5개의 단어가 추출되었다! 이제 이 5개 단어를 feature로 삼아서 확률 $p(a_i \mid S = T)$와 $p(a_i \mid S = F)$를 각각 구해보자.

```py
# derive spam probability
X_train_spam = X_train[y_train == 1]
print('[train] # of spam', len(X_train_spam))

spam_count = {token: 0 for token in target_tokens}

for text in X_train_spam:
  tokens = tokenize(text)

  for target_token in target_tokens:
    if target_token in tokens:
      spam_count[target_token] += 1

print('spam count:', spam_count)

word_spam_prob = {token: spam_count[token] / len(X_train_spam) for token in spam_count}
print("P(w_i | S = T)", word_spam_prob) # P(w_i | S = T)
...
[train] # of spam 608
spam count: {'call': 263, 'know': 17, 'free': 132, 'like': 12, 'good': 10}
P(w_i | S = T) {'call': 0.43256578947368424, 'know': 0.027960526315789474, 'free': 0.21710526315789475, ...
```

ham의 경우도 비슷하게 구하면 된다.

이제 $p(c_k)$에 대응하는 $p(S = T)$, $p(S = F)$를 구해보자.

```py
spam_prob = len(X_train_spam) / len(X_train)
ham_prob = 1 - spam_prob
print("spam_prob", spam_prob) # P(S = T)
print("ham_prob", ham_prob) # P(S = F)
```

<br/>

이제 준비는 다 되었다! 실제로 train, test-set에 적용해보자!

```py
# apply naive bayes for train-set
correct_count = 0

for idx, text in X_train.items():
  tokens = tokenize(text)
  y_gt = y_train[idx]

  # spam case
  spam_prob_ = spam_prob
  for token in tokens:
    if token in target_tokens:
      spam_prob_ *= word_spam_prob[token]

  # ham case
  ham_prob_ = ham_prob
  for token in tokens:
    if token in target_tokens:
      ham_prob_ *= word_ham_prob[token]

  y_pred = ham_prob < spam_prob
  correct_count += (y_gt == y_pred)

print(correct_count)
print(correct_count / len(X_train))
...
3849
0.863585371326004
```

test-set에 

