---
title: "Seminar 6: Naive Bayes Classifier"
layout: post
use_math: true
tags: ["seminar"]
---

<br/>

ì—¬ëŸ¬ë¶„ì´ [HW5]({{"/2021/12/21/hw-5.html" | relative_url}})ì˜ ê³¼ì œë¥¼ ì—´ì‹¬íˆ í–ˆë‹¤ë©´, ìì—°ì–´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ "í† í°í™”(tokenization)"ì„ ì˜ ì´í•´í–ˆì„ ê²ƒì´ë‹¤ ğŸ‘

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì‹¤ì œ ë°ì´í„°ì—ì„œ í† í°í™”ë¥¼ ì ìš©í•˜ê³ , ê°„ë‹¨í•œ ML ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ê² ë‹¤.

<hr/>

## Spam Classifier

[SNS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)ì—ëŠ” 5ì²œê°œ ì •ë„ì˜ ì´ë©”ì¼ ë°ì´í„°ê°€ ìˆë‹¤. 


```text
     v1                                                 v2
0   ham  Go until jurong point, crazy.. Available only ...
1   ham                      Ok lar... Joking wif u oni...
2  spam  Free entry in 2 a wkly comp to win FA Cup fina...
3   ham  U dun say so early hor... U c already then say...
4   ham  Nah I don't think he goes to usf, he lives aro...
```

ë©”ì¼ ë³¸ë¬¸ê³¼ í•¨ê»˜ ìŠ¤íŒ¸(spam) ë©”ì¼ê³¼ ì¼ë°˜(ham) ë©”ì¼ë¡œ ë ˆì´ë¸”ì´ ìˆëŠ”ë°, ì´ ë°ì´í„°ìœ¼ë¡œ ìŠ¤íŒ¸ì¸ì§€ ì•„ë‹Œì§€ ë¶„ë¥˜í•˜ëŠ” ê°„ë‹¨í•œ ë¶„ë¥˜ê¸°ë¥¼ êµ¬í˜„í•´ë³´ê² ë‹¤.

## Naive Bayes Classifier

ë°ì´í„°ë¥¼ ì„œë¡œ ë‹¤ë¥¸ ë ˆì´ë¸”ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¥¼ í‘¸ëŠ” classifierì—ëŠ” ì—¬ëŸ¬ê°€ì§€ ì¢…ë¥˜ê°€ ìˆë‹¤. 

- Logistic Regression
- KNN Classifier
- Naive Bayes Classifier
- Decision-tree based classifiers...

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ê·¸ ì¤‘ì—ì„œ \<Naive Bayes classifier\>ë¼ëŠ” ë¶„ë¥˜ê¸°ë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³¼ ê²ƒì´ë‹¤.

### ì´ë¡ 

ë¨¼ì € Naive Bayesì˜ ì´ë¡ ì„ í•˜ë‚˜ì”© ì‚´í´ë³´ì.

NBì˜ ëª©í‘œëŠ” $p(c_k \mid x)$ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ í™•ë¥ ì€ ë°ì´í„° $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë ˆì´ë¸” $c_k$ì¼ í™•ë¥ ì„ ë§í•œë‹¤. ì¢€ë” í’€ì–´ì“°ë©´...

- $p(S = T \mid x)$: ë°ì´í„° $x$ê°€ ìŠ¤íŒ¸(spam)ì¼ í™•ë¥ 
- $p(S = F \mid x)$: ë°ì´í„° $x$ê°€ í–„(ham)ì¼ í™•ë¥ 

ê·¸ëŸ°ë° ìœ„ì™€ ê°™ì€ ì¡°ê±´ë¶€ í™•ë¥ ì€ ë² ì´ì¦ˆ ì •ë¦¬(Bayes Theorem)ì— ì˜í•´ ì•„ë˜ì™€ ê°™ì´ í’€ì–´ ì“¸ ìˆ˜ ìˆë‹¤.

$$
p(c_k \mid x) = \frac{p(c_k) \cdot p(x \mid c_k)}{p(x)}
$$

ë¶„ëª¨ í˜•íƒœì—ì„œ ê° í™•ë¥ ì˜ ì˜ë¯¸ë¥¼ ì‚´í´ë³´ë©´

- $p(c_k)$: ë ˆì´ë¸”ì´ $c_k$ì¼ í™•ë¥ 
  - $p(S=T)$: ìŠ¤íŒ¸ ë©”ì¼ì¼ í™•ë¥  = (ìŠ¤íŒ¸ ë©”ì¼ì˜ ìˆ˜) / (ì „ì²´ ë©”ì¼ì˜ ìˆ˜)
- $p(x \mid c_k)$: ë ˆì´ë¸”ì´ $c_k$ì¼ ë•Œ, ë°ì´í„° $x$ê°€ ë“±ì¥í•  í™•ë¥ 
- $p(x)$: ë°ì´í„° $x$ê°€ ë“±ì¥í•  í™•ë¥ 

ì´ë•Œ $p(x)$ëŠ” ìƒê°í•  í•„ìš”ê°€ ì—†ëŠ”ê²Œ, ì •í™•í•œ ê°’ì„ êµ¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ìš°ë¦¬ê°€ ì§€ê¸ˆ ë³´ëŠ” ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ ë¬¸ì œë¼ë©´, ë°ì´í„° $x$ëŠ” `"Go until jurong point, crazy.."`ë¼ëŠ” ë©”ì¼ ë³¸ë¬¸ì— í•´ë‹¹í•˜ëŠ”ë°, ì´ ë©”ì¼ ë³¸ë¬¸ì€ ì•„ì£¼ ê±°ëŒ€í•œ ì½”í¼ìŠ¤ ê³µê°„ ìœ„ì˜ í•œ ì ì— ë¶ˆê³¼í•˜ê¸° ë•Œë¬¸ì— í™•ë¥ ì´ ì •ë§ ë‚®ë‹¤. 

ë¬¼ë¡  í™•ë¥ ì„ ì •ì˜í•˜ëŠ” ì „ì²´ ê³µê°„ì„ ëª…í™•íˆ ì •ì˜í•˜ë©´ ì‹¤ì œ $p(x)$ì˜ ê°’ì„ êµ¬í•  ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì§€ê¸ˆ ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ë¬¸ì œì—ì„œëŠ” $p(x)$ ê°’ì„ ì•„ëŠ” ê²ƒì´ ì „í˜€ ì˜ë¯¸ê°€ ì—†ë‹¤.

ì¼ë‹¨ ìš°ë¦¬ëŠ” $p(c_k \mid x)$ì˜ ê°’ì´ í° ë ˆì´ë¸”ì„ ì •ë‹µìœ¼ë¡œ ì—¬ê¸¸ ê²ƒì´ë‹¤. ê·¸ë˜ì„œ

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k \mid x)
$$

ì„ êµ¬í•˜ê²Œ ë˜ëŠ”ë°, $p(S =T \mid x)$ì—ì„œë‚˜ $p(S = F \mid x)$ì—ì„œë‚˜ $p(x)$ëŠ” ê³µí†µë˜ëŠ” ìƒìˆ˜ì´ê¸° ë•Œë¬¸ì— output $y$ë¥¼ êµ¬í•˜ëŠ”ë° ì „í˜€ ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë˜ì„œ

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(x \mid c_k)
$$

ì—¬ê¸°ê¹Œì§€ê°€ NB classifierì—ì„œ 'Bayes'ë¼ëŠ” ì´ë¦„ì´ ë¶™ëŠ” ì´ìœ ë‹¤!

<br/>

ë‹¤ìŒ ë‹¨ê³„ëŠ” $p(x \mid c_k)$ë¥¼ rawí•œ ë°ì´í„° $x$ ê·¸ëŒ€ë¡œ ì“°ëŠ”ê²Œ ì•„ë‹ˆë¼ feature $(a_1, ..., a_n)$ì˜ í˜•íƒœë¡œ ê¸°ìˆ í•˜ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ë©”ì¼ ë³¸ë¬¸ì—ì„œ ëª‡ê°€ì§€ íŠ¹ì§•ì ì¸ ë¶€ë¶„ë“¤ì„ ì •í•  ìˆ˜ ìˆë‹¤. `'free'`ë¼ëŠ” ë‹¨ì–´ì˜ ìœ ë¬´, `'Call'`ë¼ëŠ” ë‹¨ì–´ì˜ ìœ ë¬´, ë³¸ë¬¸ì˜ ê¸¸ì´, ê°™ì€ ë‹¨ì–´ê°€ ìµœëŒ€ ëª‡ë²ˆ ë°˜ë³µë˜ëŠ”ì§€ ë“±ë“±... Feature Extractionì„ ìˆ˜í–‰í•˜ë©´ raw textê°€ ì•„ë‹ˆë¼ ì •ëŸ‰ì ì¸ featureì˜ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

NBì˜ ê²½ìš°ëŠ” íŠ¹ì • ë‹¨ì–´ê°€ ë“±ì¥ í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ featureë¡œ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ $p(w = \text{free} \mid S = T)$ë¼ê³  í•˜ë©´... ìŠ¤íŒ¸ì¸ ë©”ì¼ ì¤‘ì—ì„œ $\text{free}$ë¼ëŠ” ë‹¨ì–´ê°€ ë“±ì¥í•  í™•ë¥ ì„ ë§í•œë‹¤! ì´ê²ƒì€ 

$$
p(w = \text{free} \mid S = T) = \frac{\text{#. of spam mails contain word 'free'}}{\text{#. of spam mails}}
$$

ë¡œ ì‰½ê²Œ í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆë‹¤! ğŸ‘

<br/>

ì´ì œ $p(x \mid c_k)$ë¥¼ feature $(a_1, ..., a_n)$ì˜ í˜•íƒœë¡œ ê¸°ìˆ í•˜ë©´...

$$
p(x \mid c_k) = p(a_1, ..., a_n \mid c_k)
$$

ê·¸ëŸ°ë° NBëŠ” ê° í”¼ì²˜ ì‚¬ì´ì— \<naive assumption\>ì´ë¼ëŠ” ê°€ì •ì„ í•œë‹¤. ì´ê²ƒì€ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="statement" markdown="1" align="center">

"One assumption taken is the <span style="color:red;">strong independence</span> assumptions btw the features."

</div>

ì¦‰, ê° í”¼ì³ê°€ ì„œë¡œ **ë…ë¦½**ì´ë‹¤!ë¥¼ ê°€ì •í•œë‹¤. ì´ê²ƒì€ ê³§ $p(a_1, ..., a_n \mid c_k)$ì— ëŒ€í•´ ì•„ë˜ê°€ ì„±ë¦½í•¨ì„ ë§í•œë‹¤.

$$
p(a_1, ..., a_n \mid c_k)
= p(a_1 \mid c_k) p(a_2 \mid c_k) \cdots p(a_n \mid c_k)
$$

ê·¸ë˜ì„œ ê° í”¼ì²˜ì˜ í™•ë¥ ì„ ë‹¨ìˆœíˆ ê³±í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ joint probability $p(a_1, ..., a_n \mid c_k)$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤! ğŸ‘

ë¬¼ë¡  strong indepence ê°€ì •ì€ ëª¨ë¸ë§ì„ ìœ„í•´ ì ë‹¹í•œ í˜•íƒœë¡œ ê°€ì •í•œ ê²ƒì¼ ë¿ì´ë‹¤. ì‹¤ì œë¡  ê° í”¼ì²˜ê°€ ë…ë¦½ì´ ì•„ë‹ˆë¼ correlate ë˜ì–´ ìˆì„ ìˆ˜ ìˆë‹¤.

<br/>

ì! ê·¸ëŸ¼ ì›ë˜ êµ¬í•˜ë ¤ê³  í–ˆë˜ output $y$ë¥¼ ë‹¤ì‹œ êµ¬í•´ë³´ì.

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(x \mid c_k)
$$

$p(x \mid c_k)$ê°€ feature $a_i$ë¡œ ë°”ë€Œê³  \<naive assumption\>ì— ì˜í•´ ìœ„ì˜ ì‹ì€ ì•„ë˜ê°€ ëœë‹¤.

$$
\begin{aligned}
y 
&= \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(a_1 \mid c_k) \cdots p(a_n \mid c_k) \\
&= \underset{c_k}{\text{argmax}} \; p(c_k) \cdot \prod_i^n p(a_i \mid c_k)
\end{aligned}
$$

ê²°êµ­ ê° ë ˆì´ë¸” $c_k$ì— ëŒ€í•´, $S=T$, $S=F$ì— ëŒ€í•´ $p(c_k) \cdot \prod_i^n p(a_i \mid c_k)$ ê°’ì„ êµ¬í•´ ë‘ ê°’ì„ ë¹„êµí•´ ë” í° ë ˆì´ë¸”ì˜ ê°’ì„ output $y$ë¡œ ë§¤ê¸°ë©´ ë˜ëŠ” ê²ƒì´ë‹¤! ğŸ™Œ

<hr/>

### êµ¬í˜„

ì... ê·¸ëŸ¼ ì´ê±¸ ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì! ì‚¬ì‹¤ êµ¬í˜„ ìì²´ë„ ê·¸ë¦¬ ì–´ë µì§€ ì•Šìœ¼ë‹ˆ ì˜ ë”°ë¼ì™€ë³´ì ğŸ˜‰

ì¼ë‹¨ ìŠ¤íŒ¸ ë°ì´í„°ì…‹ [SNS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)ì„ ì¤€ë¹„í•œë‹¤. 

```bash
!wget https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv
```

ê·¸ë¦¬ê³  ì ë‹¹íˆ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ê³ , train-test splitì„ ìˆ˜í–‰í•˜ë©´...

```py
data = pd.read_csv('spam.csv', encoding='latin1')
print("ìƒ˜í”Œ ìˆ˜", len(data)) # 5,572
data.head()

data = data[['v1', 'v2']] # í•„ìš”í•œ ì»¬ëŸ¼í•œ ì¶”ì¶œ
data.head()

data['v1'] = data['v1'].replace(['ham','spam'],[0,1]) # labelì„ 0, 1ë¡œ ë³€ê²½
data.head()
```

```py
data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)

X_train = data_train['v2']
y_train = data_train['v1']
X_test = data_test['v2']
y_test = data_test['v1']

print('=== í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ ===')
print('ham', str(round(np.sum(y_train == 0) / len(y_train) * 100, 1)) + '%')
print('spam', str(round(np.sum(y_train == 1) / len(y_train) * 100, 1)) + '%')
print('=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ ===')
print('ham', str(round(np.sum(y_test == 0) / len(y_test) * 100, 1)) + '%')
print('spam', str(round(np.sum(y_test == 1) / len(y_test) * 100, 1)) + '%')

...

=== í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ ===
ham 86.4%
spam 13.6%
=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ ===
ham 87.5%
spam 12.5%
```

<br/>

ë©”ì¼ ë³¸ë¬¸ ìì²´ëŠ” ë„ˆë¬´ rawí•œ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì— ë°”ë¡œ ì“°ê¸° ë³´ë‹¤ëŠ” ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ”ê²Œ ì¢‹ë‹¤. [HW5]({{"/2021/12/21/hw-5.html" | relative_url}}) ê³¼ì œì—ì„œ ë°°ìš´ í† í°í™”(tokenization)ì„ í™œìš©í•´ë³´ì!

```py
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
```

``` py
def tokenize(text):
  sentences = sent_tokenize(text)
  tokens = []

  for sentence in sentences:
    # ë‹¨ì–´ í† í°í™”
    words = word_tokenize(sentence)
    for word in words:
      word = word.lower() # ì†Œë¬¸ì ë³€í™˜
      if word in stop_words: continue # ë¶ˆìš©ì–´ ì œê±°
      if len(word) <= 3: continue # ê¸¸ì´ 3 ì´í•˜ ì œê±°
      tokens.append(word)
  return tokens
```

í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ í† í°í™”ì™€ ë‹¨ì–´ í† í°í™”ë¥¼ í•œ í›„, ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  ê¸¸ì´ 3 ì´í•˜ì˜ ë‹¨ì–´ë“¤ì„ ëª¨ë‘ ì œê±°í–ˆë‹¤!

ê·¸ë¦¬ê³  `vocab = {}`ì— ê° í† í°ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì €ì¥í•˜ë©´...

```py
vocab = {}
for text in X_train:
  # í† í°í™”
  tokens = tokenize(text)

  for token in tokens:
    if token not in vocab:
      vocab[token] = 0
    vocab[token] += 1

print(vocab)
print(len(vocab))

...

{'sleeping': 10, 'feeling': 15, 'well': 86, 'come': 177, ...
7449
```

ì™€ ê°™ì´ ì¶œë ¥ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° ì´ ëª¨ë“  í† í°ì„ ë‹¤ ì“¸ ê±´ ì•„ë‹ˆê³  ë¹ˆë„ìˆ˜ Top 5ì˜ ë‹¨ì–´ì— ëŒ€í•´ì„œë§Œ feature extractionì„ ì ìš©í•  ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì ë‹¹íˆ ì •ë ¬í•´ `target_token`ì„ ì •í•˜ë©´...

```py
# filter out top 5 tokens
vocab_sorted = sorted(vocab.items(), key=lambda x : x[1], reverse=True)

vocab_size = 5
vocab_sorted = vocab_sorted[:vocab_size]
print(vocab_sorted)
target_tokens = [word for word, _ in vocab_sorted]
print(target_tokens)

...

[('call', 460), ('know', 216), ('free', 212), ('like', 204), ('good', 193)]
['call', 'know', 'free', 'like', 'good']
```

ìœ„ì™€ ê°™ì´ 5ê°œì˜ ë‹¨ì–´ê°€ ì¶”ì¶œë˜ì—ˆë‹¤! ì´ì œ ì´ 5ê°œ ë‹¨ì–´ë¥¼ featureë¡œ ì‚¼ì•„ì„œ í™•ë¥  $p(a_i \mid S = T)$ì™€ $p(a_i \mid S = F)$ë¥¼ ê°ê° êµ¬í•´ë³´ì.

```py
# derive spam probability
X_train_spam = X_train[y_train == 1]
print('[train] # of spam', len(X_train_spam))

spam_count = {token: 0 for token in target_tokens}

for text in X_train_spam:
  tokens = tokenize(text)

  for target_token in target_tokens:
    if target_token in tokens:
      spam_count[target_token] += 1

print('spam count:', spam_count)

word_spam_prob = {token: spam_count[token] / len(X_train_spam) for token in spam_count}
print("P(w_i | S = T)", word_spam_prob) # P(w_i | S = T)
...
[train] # of spam 608
spam count: {'call': 263, 'know': 17, 'free': 132, 'like': 12, 'good': 10}
P(w_i | S = T) {'call': 0.43256578947368424, 'know': 0.027960526315789474, 'free': 0.21710526315789475, ...
```

hamì˜ ê²½ìš°ë„ ë¹„ìŠ·í•˜ê²Œ êµ¬í•˜ë©´ ëœë‹¤.

ì´ì œ $p(c_k)$ì— ëŒ€ì‘í•˜ëŠ” $p(S = T)$, $p(S = F)$ë¥¼ êµ¬í•´ë³´ì.

```py
spam_prob = len(X_train_spam) / len(X_train)
ham_prob = 1 - spam_prob
print("spam_prob", spam_prob) # P(S = T)
print("ham_prob", ham_prob) # P(S = F)
```

<br/>

ì´ì œ ì¤€ë¹„ëŠ” ë‹¤ ë˜ì—ˆë‹¤! ì‹¤ì œë¡œ train, test-setì— ì ìš©í•´ë³´ì!

```py
# apply naive bayes for train-set
correct_count = 0

for idx, text in X_train.items():
  tokens = tokenize(text)
  y_gt = y_train[idx]

  # spam case
  spam_prob_ = spam_prob
  for token in tokens:
    if token in target_tokens:
      spam_prob_ *= word_spam_prob[token]

  # ham case
  ham_prob_ = ham_prob
  for token in tokens:
    if token in target_tokens:
      ham_prob_ *= word_ham_prob[token]

  y_pred = ham_prob < spam_prob
  correct_count += (y_gt == y_pred)

print(correct_count)
print(correct_count / len(X_train))
...
3849
0.863585371326004
```

test-setì— 

