---
title: "Seminar 8: Word Embedding"
layout: post
use_math: true
tags: ["seminar"]
---

<br/>

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë‹¨ì–´(word)ë¥¼ ë²¡í„°í™”í•˜ëŠ” ë°©ë²•ì¸ **<u>ì›Œë“œ ì„ë² ë”©(Word Embedding)</u>**ì— ëŒ€í•´ ë‹¤ë£¬ë‹¤. ì´ì „ì˜ ë¬¸ì ì¸ì½”ë”©ê³¼ ë‹¬ë¦¬ ì›Œë“œ ì„ë² ë”©ì€ NLP ë¶„ì•¼ì—ì„œ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì´ ë¶€ë¶„ì„ ì˜ ì´í•´í•´ì•¼ ì´í›„ì— ë‹¤ë£° NLP ëª¨ë¸ì„ ì´í•´í•˜ëŠ”ë° ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

- Word Embeddingì´ë€?
  - í¬ì†Œ í‘œí˜„ê³¼ ë°€ì§‘ í‘œí˜„
- Word2Vec
  - ë¶„ì‚° í‘œí˜„
  - Continuous Bag of Words
- WordRNN
  - nn.Embedding

<hr/>

## Word Embeddingì´ë€?

ë¬¸ì ì¸ì½”ë”©ì—ì„œëŠ” ì½”í¼ìŠ¤(corpus)ì—ì„œ vocab setì„ êµ¬í•œ í›„ vocab size $V$ì˜ í¬ê¸°ë¡œ one-hot encoddingì„ ìˆ˜í–‰í–ˆë‹¤. character-levelì—ì„œì•¼ ì•ŒíŒŒë²³ì€ 26ê°œë¿ì´ë‹ˆ one-hot vector í¬ê¸°ê°€ ê·¸ë ‡ê²Œ ë¶€ë‹´ë˜ì§€ëŠ” ì•Šì•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¨ì–´(word) ìˆ˜ì¤€ì—ì„œëŠ” ê°€ëŠ¥í•œ ì¡°í•©ì´ ë„ˆë¬´ ë§ê¸° ë•Œë¬¸ì— vocab size $V$ê°€ ë¬´ì§€ë¬´ì§€ í¬ë‹¤.

```py
# US National Anthem
text = f'''O say can you see, by the dawnâ€™s early light,
What so proudly we hailâ€™d at the twilightâ€™s last gleaming,
Whose broad stripes and bright stars through the perilous fight
Oâ€™er the ramparts we watchâ€™d were so gallantly streaming?
And the rocketâ€™s red glare, the bombs bursting in air,
Gave proof through the night that our flag was still there,
O say does that star-spangled banner yet wave
Oâ€™er the land of the free and the home of the brave?'''

character vocab size: ë§ì•„ë´ì•¼ 26+
word vocab size: í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì§€ë©´ ë¬´í•œí•˜ê²Œ ì¦ê°€... 
```

ê·¸ë˜ì„œ ë‹¨ì–´ ìˆ˜ì¤€ì—ì„œëŠ” ë”ì´ìƒ one-hot encodingì€ ì‚¬ìš©í•  ìˆ˜ ì—†ê²Œ ëœë‹¤. ë˜, one-hot encodingìœ¼ë¡œëŠ” ë‹¨ì–´ì˜ ìœ ì‚¬ë„(similarity)ë¥¼ ì¸¡ì •í•  ìˆ˜ ì—†ë‹¤ëŠ” ë‹¨ì ë„ ìˆë‹¤. ('ê°œ'ì™€ 'ê³ ì–‘ì´'ë¼ëŠ” ë‹¨ì–´ëŠ” ë™ë¬¼ì´ë¼ëŠ” ê´€ì ì—ì„œ 'í† ë§ˆí† 'ë³´ë‹¤ëŠ” ìœ ì‚¬í•œ ë‹¨ì–´ì´ë‹¤. ê·¸ëŸ¬ë‚˜ one-hot encodingìœ¼ë¡œëŠ” ì´ ìœ ì‚¬ë„ë¥¼ ì „í˜€ ê³„ì‚°í•  ìˆ˜ ì—†ë‹¤.)

ì´ëŸ° one-hot encodingì˜ **<u>í¬ì†Œ í‘œí˜„(sparse representation)</u>**ì— ë°˜ëŒ€ë˜ëŠ” ê°œë…ìœ¼ë¡œ **<u>ë°€ì§‘ í‘œí˜„(dense representation)</u>**ì´ ìˆë‹¤. one-hotê³¼ ê°™ì€ í¬ì†Œ í‘œí˜„ì´ vocab size $V$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ encoding vectorì˜ í¬ê¸°ë¥¼ ê²°ì •í–ˆë‹¤ë©´, ë°€ì§‘ í‘œí˜„ì€ ê³ ì •ëœ í¬ê¸°ì˜ (ì˜ˆë¥¼ ë“¤ì–´ 5) ë²¡í„°ë¡œ ë‹¨ì–´ë¥¼ encodingí•œë‹¤.

```py
dog = [0, 0, 0, ..., 1, 0, 0, ... , 0] # 10,000 ì°¨ì›ì˜ one-hot
vs.
dog = [1.1, 3.2, -0.5, 8.0, -3.3] # size 5ì˜ ë°€ì§‘ í‘œí˜„!
```

ì´ë ‡ê²Œ ë‹¨ì–´ë¥¼ ë°€ì§‘ í‘œí˜„ìœ¼ë¡œ encoding í•˜ë©´ ê³µê°„ìƒì˜ ì´ë“ë„ ì–»ê³ , ì´í›„ì— ë‘ ë‹¨ì–´ì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ì‘ì—…ë„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤!

ìœ„ì™€ ê°™ì´ ë‹¨ì–´ë¥¼ ë°€ì§‘ í‘œí˜„ìœ¼ë¡œ encoding í•˜ëŠ” ê²ƒì„ **<u>ì›Œë“œ ì„ë² ë”©(Word Embedding)</u>**ì´ë¼ê³  í•œë‹¤. ê·¸ë¦¬ê³  ì´ë•Œì˜ encoding vectorë¥¼ **<u>ì„ë² ë”© ë²¡í„°(Embedding Vector)</u>**ë¼ê³  í•œë‹¤.

<hr/>

## Word2Vec

**<u>Word2Vec</u>**ì€ ì›Œë“œ ì„ë² ë”© ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤. Word2Vecì€ ë‹¨ì–´ë¥¼ ì„ë² ë”© í•˜ê¸° ìœ„í•´ **<u>ë¶„ì‚° í‘œí˜„(distributed representation)</u>**ì´ë¼ëŠ” ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤.

ë¶„ì‚° í‘œí˜„ì€ <span class="half_HL">"ë¹„ìŠ·í•œ ë¬¸ë§¥ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤"</span>ë¼ëŠ” ë¶„í¬ ê°€ì„¤(distributional hypothesis)ì„ ë°”íƒ•ìœ¼ë¡œ í•œë‹¤.

ì´í•˜ ìì„¸í•œ ë‚´ìš©ì€ "ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì–¸ì–´ ì²˜ë¦¬ ì…ë¬¸"ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

ğŸ‘‰ [ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì–¸ì–´ ì²˜ë¦¬ ì…ë¬¸: ë¶„ì‚° í‘œí˜„ê³¼ Continuous Bag of Words](https://wikidocs.net/22660#:~:text=2.%20%EB%B6%84%EC%82%B0%20%ED%91%9C%ED%98%84(Distributed%20Representation)) ([w/ í•˜ì´ë¼ì´íŒ…](https://share.getliner.com/RuR5I4rQf9))

(ê°„ë‹¨ ìš”ì•½)

- Word2Vecì€ Dense Represetationì´ë©´ì„œ Distributed Represenationì´ë‹¤
  - "ë¹„ìŠ·í•œ ë¬¸ë§¥ì— ìˆëŠ” ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤."
- CBOWì€ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì–•ì€ ì‹ ê²½ë§ì„ í•™ìŠµí•´ Word2Vec í•˜ëŠ” ë°©ì‹ì´ë‹¤.
- ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µì´ ì›í•« ì¸ì½”ë”©ìœ¼ë¡œ ë˜ì–´ ìˆê³ , í•™ìŠµí•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ Embedding vectorë¡œ ì‚¬ìš©í•œë‹¤.

<hr/>

## `nn.Embedding()`ê³¼ WordRNN

PyTorchì—ì„œëŠ” `nn.Embeding()`ì„ ì‚¬ìš©í•´ Word Embeddingì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ, ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ë‹¨ì–´ë“¤ì€ ëª¨ë‘ **<u>ì •ìˆ˜ ì¸ì½”ë”©</u>** ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤! (CBoWì—ì„œëŠ” ì…ì¶œë ¥ ê°’ì´ one-hot encoding ë˜ì–´ ìˆì–´ì•¼ í–ˆë‹¤!)

`nn.Embedding()`ì€ CBoWì™€ëŠ” ì „í˜€ ë‹¤ë¥¸ ëª¨ë¸ì´ë‹¤. `nn.Embedding()`ì€ ë‹¨ìˆœíˆ ë£©ì—… í…Œì´ë¸”(lookup table) í•˜ë‚˜ë§Œ ìˆëŠ” êµ¬ì¡°ë¡œ ì •ìˆ˜ê°€ ì…ë ¥ë˜ë©´ í•´ë‹¹ ì •ìˆ˜ì— í•´ë‹¹í•˜ëŠ” ë£©ì—… í…Œì´ë¸”ì˜ rowë¥¼ ë°˜í™˜í•  ë¿ì´ë‹¤. ê·¸ë˜ì„œ ì´ˆê¸°ì˜ `nn.Embedding()`ì˜ ì¶œë ¥ê°’ì€ ë‹¨ìˆœíˆ ì •ìˆ˜ë¥¼ ë²¡í„°ë¡œ ë§¤í•‘í•œ ê²ƒì´ ë¶ˆê³¼í•˜ë©´ ì‹¤ì§ˆì ì¸ word embeddingì´ ì•„ë‹ˆë‹¤. ë‹¤ë§Œ, ì „ì²´ ëª¨ë¸ì´ í•™ìŠµë˜ëŠ” ê³¼ì •ì—ì„œ ë£©ì—… í…Œì´ë¸”ì˜ ê°’ì´ ì¡°ì •ë˜ëŠ”ë°, í•™ìŠµì´ ëë‚œ í›„ì˜ ë£©ì—…í…Œì´ë¸”ì˜ ê°’ì´ ë³¸ë˜ ì›í–ˆë˜ ì‹¤ì§ˆì ì¸ word embeddingì´ë‹¤.

![](https://wikidocs.net/images/page/33793/lookup_table.PNG)

<div class="statement" markdown="1">

Q. ê·¸ëŸ¼ë‹´... `nn.Embedding()`ì€ ë‹¨ìˆœíˆ 'ë£©ì—…í…Œì´ë¸”'ì´ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ í…ì„œì¼ ë¿ì¸ê±´ê°€?

A. ë†€ëê²Œë„ ê·¸ë ‡ë‹¤. 

</div>

<br/>

PyTorchì˜ `nn.Embeding()`ì„ í™œìš©í•´ word embeddingì„ ìˆ˜í–‰í•˜ê³  ì´ë¥¼ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” WordRNNì„ êµ¬í˜„í•´ë³´ì.

ì˜ˆì‹œ ë¬¸ì¥ì€ "Fly me to the moon"ì´ë¼ëŠ” ê³¡ì˜ ê°€ì‚¬ì´ë‹¤.

```py
text = f"""
Fly me to the moon
Let me play among the stars
And let me see what spring is like
On a-Jupiter and Mars
In other words, hold my hand
In other words, baby, kiss me

Fill my heart with song
And let me sing forevermore
You are all I long for
All I worship and adore
In other words, please be true
In other words, I love you
"""
```

`CharRNN` ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ëª‡ê°€ì§€ pre-processingì„ ê±°ì³ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ë§Œë“ ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ì „ì˜ `CharRNN` ë•Œì™€ëŠ” ë‹¬ë¦¬ ì›-í•« ì¸ì½”ë”©ì´ ì•„ë‹ˆë¼ **ì •ìˆ˜ ì¸ì½”ë”©**ì„ í•œë‹¤.


```py
# ì½”ë“œëŠ” ìƒëµí•˜ê³  ê²°ê³¼ë§Œ ì œì‹œ
0 ['Fly', 'me', 'to', 'the', 'moon'] -> ['me', 'to', 'the', 'moon', 'Let']
10 ['stars', 'And', 'let', 'me', 'see'] -> ['And', 'let', 'me', 'see', 'what']
20 ['a-Jupiter', 'and', 'Mars', 'In', 'other'] -> ['and', 'Mars', 'In', 'other', 'words,']
30 ['other', 'words,', 'baby,', 'kiss', 'me'] -> ['words,', 'baby,', 'kiss', 'me', 'Fill']
40 ['And', 'let', 'me', 'sing', 'forevermore'] -> ['let', 'me', 'sing', 'forevermore', 'You']
50 ['for', 'All', 'I', 'worship', 'and'] -> ['All', 'I', 'worship', 'and', 'adore']
60 ['be', 'true', 'In', 'other', 'words,'] -> ['true', 'In', 'other', 'words,', 'I']
[4, 2, 44, 19, 18]
[2, 44, 19, 18, 12]
X torch.Size([63, 5])
Y torch.Size([63, 5])
```

<br/>

PyTorchì˜ `nn.Embedding()`ì€ 'ì „ì²´ ë‹¨ì–´ ê°¯ìˆ˜'ì™€ 'ì„ë² ë”© ë²¡í„°ì˜ í¬ê¸°'ë¥¼ `num_embeddings`ì™€ `embeeding_dim`ìœ¼ë¡œ ì…ë ¥ ë°›ëŠ”ë‹¤.

```py
embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=3)
print(embedding_layer)
---
Embedding(48, 3)
```

ì•ì—ì„œ ë§Œë“¤ì–´ë‘” `X` í…ì„œë¡œ ì¶œë ¥ì„ í™•ì¸í•˜ë©´...

```py
out = embedding_layer(X)
print('X', X.shape)
print('out', out.shape)
---
X torch.Size([63, 5])
out torch.Size([63, 5, 3])
```

<br/>

ì! ì´ì œ ì´ `nn.Embedding()`ì„ í™œìš©í•´ `WordRNN` ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ì.

```py
class WordRNN(nn.Module):
  def __init__(self, vocab_size, embedding_dim=3, hidden_dim=20, num_layers=2):
    super(WordRNN, self).__init__()
    self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
    self.rnn = nn.RNN(embedding_dim, embedding_dim, num_layers=num_layers, batch_first=True)
    self.hidden = nn.Linear(embedding_dim, hidden_dim, bias=True)
    self.fc = nn.Linear(hidden_dim, vocab_size, bias=True)
  
  def forward(self, x):
    embedding = self.embedding_layer(x)
    x, _status = self.rnn(embedding)
    x = self.hidden(x)
    x = self.fc(x)
    return x
```

ì •ìˆ˜ê°’ìœ¼ë¡œ ì…ë ¥ë˜ëŠ” ë¬¸ìë¥¼ `nn.Embedding()`ìœ¼ë¡œ ë²¡í„°í™” í•œ í›„, `nn.RNN()`ì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ë‹¤ìŒ ë‹¨ì–´(embedding vector í˜•íƒœë¡œ ì¸ì½”ë”©)ë¥¼ ì˜ˆì¸¡í•œë‹¤. ì´í›„ `hidden`ê³¼ `fc` ë ˆì´ì–´ë¥¼ ê±°ì³ ì „ì²´ ë‹¨ì–´ìˆ˜ ë§Œí¼ì˜ í¬ê¸°ë¥¼ ê°–ëŠ” í…ì„œë¡œ ë³€í™˜í•œë‹¤. ì´ëŠ” ì¼ì¢…ì˜ word embeddingì„ ë‹¤ì‹œ decoding í•˜ëŠ” ê³¼ì •ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” `CharRNN`ì²˜ëŸ¼ ë¶„ë¥˜ ë¬¸ì œë¥¼ í’€ë„ë¡ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë©´ ëœë‹¤ ğŸ™Œ

```py
# hyper-parameter
learning_rate = 0.1
dict_size = len(word_set)

model = WordRNN(len(word_set), embedding_dim=7, hidden_dim=20)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), learning_rate)

def textimize_result(results):
  predict_str = text.split()[0] + ' ' # start from first word

  for j, result in enumerate(results):
    if j == 0: # At first time, bring all results
      predict_str += ' '.join([index2word[int(t)] for t in result])
    else: # append only last word
      predict_str += (' ' + index2word[int(result[-1])])
  print(predict_str)

epoch = 51

for i in range(1, epoch):
  optimizer.zero_grad()
  
  outputs = model(X) # (batch, timesteps, dict_size)
  loss = criterion(outputs.reshape(-1, dict_size), Y.view(-1))
  
  loss.backward()
  optimizer.step()

  results = outputs.argmax(dim = 2) 

  if i % 5 == 0:
    print("------ %d ------" % i)
    textimize_result(results)
```

<hr/>

## ë§ºìŒë§

![](https://miro.medium.com/max/1074/1*cozibGuv9jX8bheqyirEvA.png)

ì´ë²ˆ ì£¼ì œì˜€ë˜ word2vecì€ NLP ëª¨ë¸ì˜ ìµœì‹  íŠ¸ë Œë“œë¥¼ ì‚´í´ë³´ëŠ” ì²« ê±¸ìŒì´ì˜€ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëª¨ë¸ì„ ë°°ìš°ëŠ” ê²ƒë„ ì¤‘ìš”í•˜ì§€ë§Œ, ë” ì¤‘ìš”í•œ ê²ƒì€ ë°°ìš´ ê²ƒì€ ë³¸ì¸ ë§Œì˜ í”„ë¡œì íŠ¸ë¡œ ì ìš©í•´ë³´ëŠ” ê±°ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. NLP ë¶„ì•¼ê°€ íŠ¹íˆ íƒ€ê²Ÿìœ¼ë¡œ ì‚¼ì€ íƒœìŠ¤í¬ë‚˜ ì–¸ì–´(language)ì— ë”°ë¼ ì‚¬ìš©ë˜ëŠ” í…Œí¬ë‹‰ì´ ë‹¤ì–‘í•´ì„œ í•˜ë‚˜ì˜ ì„¸ë¯¸ë‚˜ë¡œëŠ” ëª¨ë“  í…Œí¬ë‹‰ì„ ì»¤ë²„í•˜ëŠ”ê²Œ ì‰½ì§€ ì•Šì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤. NLP ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆë‹¤ë©´ Standorì˜ [CS224N: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/) ê°•ì¢Œë„ ë³‘í–‰í•´ì„œ ë³¸ ì„¸ë¯¸ë‚˜ë¥¼ ë“£ëŠ” ê±¸ ì¶”ì²œí•©ë‹ˆë‹¤.

ë‹¤ìŒ ì„¸ë¯¸ë‚˜ì—ì„œëŠ” seq2seqì™€ Attention ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•´ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤ ğŸ™

<hr/>

## References

- ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì–¸ì–´ ì²˜ë¦¬ ì…ë¬¸
  - [ì›Œë“œ ì„ë² ë”©](https://wikidocs.net/33520)
  - [Word2Vec](https://wikidocs.net/22660)
- PyTorchë¡œ ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ ì…ë¬¸
- [PyTorch - nn.Embedding()](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)