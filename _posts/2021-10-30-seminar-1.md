---
title: "Seminar 1"
layout: post
use_math: true
tags: ["seminar"]
---


### í‚¤ì›Œë“œ

- í…ì„œ(tensor)
- Design Simple Regression
- Torch.Autograd
- `nn.Linear()`

<hr/>

### ë„ì…ë§

PyTorch ì„¸ë¯¸ë‚˜ëŠ” [PyTorch ê³µì‹ íŠœí† ë¦¬ì–¼](https://pytorch.org/tutorials/)ì„ ë°”íƒ•ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. PyTorchì˜ ê³µì‹ íŠœí† ë¦¬ì–¼ì€ ë‹¤ì–‘í•œ ì˜ˆì œì™€ PyTorchì˜ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ì„œë„ ê¹Šê²Œ ë‹¤ë£¨ê¸° ë•Œë¬¸ì— ë”¥ëŸ¬ë‹ ê°œë°œìë¼ë©´ ì‹œê°„ì„ ë“¤ì—¬ ë‚´ìš© ì „ì²´ë¥¼ ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. (ê³µì‹ íŠœí† ë¦¬ì–¼ ë¦¬ë”©ì€ ê¾¸ì¤€íˆ HWë¡œ ë‚˜ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤.)

<hr/>

## Tensor

PyTorchì—ì„œ í…ì„œ(tensor)ëŠ” nì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ìë£Œí˜•ì´ë‹¤. ì•„ë˜ì™€ ê°™ì´ ì„ ì–¸ ë° ì´ˆê¸°í™” í•  ìˆ˜ ìˆë‹¤.

``` py
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)
```

ë˜ëŠ” numpy ë°°ì—´ë¡œë„ ê°€ëŠ¥í•˜ë‹¤.

```py
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
```

í…ì„œë¥¼ ìƒì„±í•˜ë©´ `.shape`, `.dtype`ì„ í†µí•´ ì°¨ì›ê³¼ ë°ì´í„°íƒ€ì…ì„ ì•Œ ìˆ˜ ìˆë‹¤.

ëœë¤ í…ì„œë‚˜ 0 ë˜ëŠ” 1ë¡œ ì´ˆê¸°í™”ëœ í…ì„œëŠ” `rand()`, `ones()`, `zeros()` í•¨ìˆ˜ë¡œ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

```py
shape = (2, 3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)
```

`.device`ëŠ” í…ì„œì—ì„œ ì²´í¬í•´ì•¼ í•  ì¤‘ìš”í•œ ì†ì„±ì´ë‹¤. í…ì„œë¥¼ ì–´ë””ì—ì„œ ì²˜ë¦¬í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤.

```py
x_data.device
```
```
# ì¶œë ¥
device(type='cpu')
```

í…ì„œë¥¼ ê·¸ëƒ¥ ìƒì„±í–ˆë‹¤ë©´ `cpu` deviceì— ë°°ì •ëœë‹¤. `gpu`ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” `.to('cuda')`ë¥¼ í†µí•´ í…ì„œë¥¼ `gpu`ë¡œ ì˜®ê²¨ì¤˜ì•¼ í•œë‹¤. ì½”ë©ì—ì„œ ì‘ì—…í•˜ê³  ìˆë‹¤ë©´ "ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU"ë¡œ ì„¤ì •í•´ì¤˜ì•¼ ì½”ë©ì˜ GPUë¥¼ ì“¸ ìˆ˜ ìˆë‹¤.

```py
# We move our tensor to the GPU if available
if torch.cuda.is_available():
  x_data = x_data.to('cuda')
  print(f"Device tensor is stored on: {x_data.device}")
```

```
# ì¶œë ¥
Device tensor is stored on: cuda:0
```

PyTorchì˜ í…ì„œëŠ” Numpy ë°°ì—´ì˜ ì—°ì‚°ë“¤(`.mul(), *, matmul(), @`)ì„ ì“¸ ìˆ˜ ìˆë‹¤. ì´ ë‚´ìš©ì€ ë„ˆë¬´ ì‰¬ì›Œì„œ íŒ¨ìŠ¤í•˜ê² ë‹¤.

<br/>

**<u>In-place operations</u>**

í…ì„œ ì—°ì‚°ì˜ ë’¤ì— ì–¸ë”ë°”(`_`)ë¥¼ ë¶™ì´ë©´ in-place ì—°ì‚°ì´ ëœë‹¤.

```py
x_data.t_()
x_data.copy_(y_data)
x_data.add_(5)
```

PyTorch ì½”ë“œì— ì¢…ì¢… ë“±ì¥í•˜ê¸°ì— ê·¸ë ‡êµ¬ë‚˜ í•˜ê³  ë„˜ì–´ê°€ë©´ ëœë‹¤. ê·¸ëŸ¬ë‚˜ ì•„ë˜ì™€ ê°™ì€ ì´ìŠˆê°€ ìˆê¸° ë•Œë¬¸ì— ê¶Œì¥í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.

> In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.

<hr/>

## Design Simple Regression

PyTorch í…ì„œë¡œ ê°„ë‹¨í•œ íšŒê·€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì. ë¨¼ì € ë°ì´í„°ë¥¼ ìƒì„±í•˜ì.

```py
X = np.arange(100)
y = 2 * X + 1

print(X, y)
```

ê·¸ë¦¬ê³  ì´ê±¸ Tensorë¡œ ë°”ê¾¼ë‹¤.

```py
X_data = torch.tensor(X)
y_data = torch.tensor(y)
print(X_data, y_data)
```

ìš°ë¦¬ëŠ” $y = w * x + b$ ê¼´ì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì„ ë§Œë“¤ ê²ƒì´ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ `W`, `b` í…ì„œë„ ìƒì„±í•œë‹¤. ì´ë“¤ì€ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°(parameter)ë¡œ back-propì˜ ëŒ€ìƒì´ ëœë‹¤. ì–´ë–¤ ê°’ìœ¼ë¡œ ì´ˆê¸°í™” í•˜ë“  ìƒê´€ ì—†ìœ¼ë‹ˆ ì¼ë‹¨ì€ `ones()`ë¡œ ì´ˆê¸°í™”í•˜ì.

```py
W = torch.ones(1)
b = torch.ones(1)
print(W, b)
```

ê·¸ë¦¬ê³  ëª¨ë¸ì„ ì½”ë“œë¡œ í‘œí˜„í•œë‹¤. `y_pred`ì— ê²°ê³¼ë¥¼ ë‹´ì.

```py
y_pred = W * X_data + b
y_pred
```

íšŒê·€ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ lossë¥¼ ì •ì˜í•œë‹¤. 

```py
loss = ((y_pred - y_data)**2).sum()
print(loss)
```

ì! ì—¬ê¸°ê¹Œì§€ í–ˆìœ¼ë©´ ì´ì œ back-propë¥¼ ìˆ˜í–‰í•˜ë©´ ëœë‹¤! ğŸ‘ ê·¸ëŸ°ë° back-propì„ ì–´ë–»ê²Œ í• ê¹Œ? computational graphë¥¼ êµ¬í˜„í•´ì•¼ í•˜ë‚˜? ë¯¸ë¶„í•´ë¥¼ êµ¬í•´ì•¼ í•˜ë‚˜? ê±±ì •í•˜ì§€ ë§ˆë¼ PyTorchì—ì„œ back-propì„ ë‹¤ êµ¬í˜„í•´ë’€ë‹¤. ê·¸ê²ƒì´ ë°”ë¡œ `Torch.autograd`ì´ë‹¤.

## Torch.autograd

ì´ë²ˆ ë¬¸ë‹¨ì—ì„œ ë‹¤ë£¨ëŠ” `autograd`ëŠ” ë”¥ëŸ¬ë‹ì˜ **Back-propagation**ì„ ì½”ë“œë¡œ êµ¬í˜„í•œ PyTorchì˜ ê¸°ëŠ¥ì´ë‹¤. back-propagationì„ ì œëŒ€ë¡œ ì´í•´í–ˆë‹¤ë©´ ì´ ë¶€ë¶„ë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

> ì™„ë²½í•˜ê²Œ ì•Œì§€ ëª»í•´ë„ ì“¸ ìˆ˜ëŠ” ìˆë‹¤. - ìƒí™œì½”ë”© 

`autograd`ì— ëŒ€í•´ ìì„¸íˆ ë‹¤ë£¨ê¸° ì „ì— ì•ì˜ íšŒê·€ ë¬¸ì œë¥¼ ë¨¼ì € í•´ê²°í•˜ì.

ë¨¼ì € ìœ„ì˜ ì½”ë“œì—ì„œ `W`, `b`ë¥¼ ì •ì˜í•œ ë¶€ë¶„ì„ ì•„ë˜ì˜ ì½”ë“œë¡œ ë‹¤ì‹œ ì“°ì.

```py
W = torch.ones(1, requires_grad=True)
b = torch.ones(1, requires_grad=True)
print(W, b)
```

ë³€ìˆ˜ê°€ ë‹¤ì‹œ ì •ì˜ë˜ì—ˆìœ¼ë‹ˆ ì•„ë˜ì˜ `y_pred`, `loss`ë„ ë‹¤ì‹œ í•œë²ˆ ê³„ì‚°í•œë‹¤.

```py
y_pred = W * X_data + b
loss = ((y_pred - y_data)**2).sum()
print(loss)
```

`loss`ì˜ ì¶œë ¥ì„ ë³´ë©´ ì´ì „ê³¼ëŠ” ë‹¤ë¥´ê²Œ `grad_fn=<SumBackward0>`ì´ ìƒê²¼ë‹¤! ê¶ê¸ˆì¦ì€ ë’¤ë¡œ í•˜ê³  ê³„ì† ë‚˜ì•„ê°€ì.

`loss` ë³€ìˆ˜ì— `.backward()`í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ back-propì´ ì‹¤í–‰ëœë‹¤. ~~ë„ˆë¬´ ê°„ë‹¨í•œë°?~~

```py
loss.backward()
```

í˜¸ì¶œ í›„ì— íŒŒë¼ë¯¸í„° `W`, `b`ë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ì. `.grad` ê°’ì„ ë³´ë©´ ì–´ë–¤ ê°’ì´ ìˆëŠ”ë°, ì´ê²ƒì´ parameter `W`ë¥¼ ê°±ì‹ í•˜ëŠ” **gradient** `dW`ì´ë‹¤.

ì´ì œ GDì˜ ë°©ì‹ëŒ€ë¡œ `W`, `b`ë¥¼ ê°±ì‹ í•˜ë©´, learning rate $\eta$ëŠ” `eta = 1e-6`ìœ¼ë¡œ ì„¤ì •í•˜ì.

```py
eta = 1e-6
W = W - eta * W.grad
b = b - eta * b.grad
```

ì´ì œ ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ëŠ” ì½”ë“œë¥¼ ì§œë³´ì.

```py
W = torch.ones(1)
b = torch.ones(1)
eta = 1e-6

for _ in range(10):
  W = torch.tensor(W, requires_grad = True)
  b = torch.tensor(b, requires_grad = True)
  y_pred = W * X_data + b
  loss = ((y_pred - y_data)**2).sum()
  print(loss)

  loss.backward()

  W = (W - eta * W.grad)
  b = (b - eta * b.grad)
```

```text
UserWarning: To copy construct from a tensor, it is recommended to use 
sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), 
rather than torch.tensor(sourceTensor).
```
ì´ëŸ° Warnì´ ëœ¬ë‹¤ë©´ ì´ëŒ€ë¡œ í•´ì¤˜ë„ ì¢‹ì€ë° ì¼ë‹¨ì€ ë¬´ì‹œí•˜ê³  ë„˜ì–´ê°€ì ğŸ˜‰

ì¶œë ¥ë˜ëŠ” `loss` ê°’ì„ í™•ì¸í•˜ë©´ ê°’ì´ ì ì  ì¤„ì–´ë“¤ì–´ 0ì— ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤ ğŸ‘

<br/>

ì! ì´ì œ **Torch.Autograd**ë€ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ì. ì•ì—ì„œë„ ë§í–ˆë“¯ AutogradëŠ” back-propì„ êµ¬í˜„í•œ ê¸°ëŠ¥ì´ë‹¤. `loss.backward()` í˜¸ì¶œ í•œë²ˆìœ¼ë¡œ back-propì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ í…ì„œì— `requires_grad=True` ì˜µì…˜ì„ ì¤˜ì•¼ í•œë‹¤. ì´ê²ƒì€ ì´ í…ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ê³„ì‚°ì„ íŠ¸ë˜í‚¹ í•˜ëŠ” Computational Graphë¥¼ ë§Œë“¤ë¼ëŠ” ì˜µì…˜ì„ ì£¼ëŠ” ê²ƒê³¼ ê°™ë‹¤. ì´ ì˜µì…˜ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ë©´ Autogradë¡œ back-propì„ í•  ìˆ˜ ì—†ë‹¤.

ì•ì—ì„œ `loss` í…ì„œë¥¼ ì¶œë ¥í–ˆì„ ë•Œ, `grad_fn=<SumBackward0>`ë¼ëŠ” ì†ì„±ì´ ì¶”ê°€ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤. ì´ê²ƒì€ ì´ í…ì„œê°€ ìœ ë„í•œ í•¨ìˆ˜ê°€ `.sum()`ì„ì„ ì €ì¥í•˜ëŠ” ì†ì„±ì´ë‹¤. AutogradëŠ” ëª‡ê°€ì§€ ê¸°ë³¸ í•¨ìˆ˜ì— ëŒ€í•œ gradient ì‹ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ ê°€ì§€ê³  ìˆëŠ” ê²ƒì´ë‹¤.

<br/>

**<u>Disabling Gradient Tracking</u>**

ê¸°ë³¸ì ìœ¼ë¡œ í…ì„œ ê³„ì‚°ì‹ì— `requires_grad=True`ì¸ í…ì„œê°€ ìˆë‹¤ë©´ ìë™ìœ¼ë¡œ Computation Graphë¥¼ ë§Œë“¤ì–´ Gradient Trackingì„ ìˆ˜í–‰í•œë‹¤. ê·¸ëŸ¬ë‚˜ ëª‡ëª‡ ê²½ìš°[^1]ì—ì„œëŠ” ê°•ì œë¡œ Gradient Trackingì„ ëŒ í•„ìš”ë„ ìˆë‹¤.

1\. `detach()`ë¥¼ ì‚¬ìš©í•˜ë¼.

```py
z = torch.matmul(x, w)+b
z_det = z.detach()
print(z_det.requires_grad)
```

í…ì„œì— `detach()`ë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ í…ì„œì— ëŒ€í•´ì„œëŠ” ë”ì´ìƒ Gradient Trackingì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.

2\. `torch.no_grad()`ë¥¼ ì‚¬ìš©í•˜ë¼.

```py
z = torch.matmul(x, w)+b
print(z.requires_grad)

with torch.no_grad():
    z = torch.matmul(x, w)+b
print(z.requires_grad)
```

`torch..no_grad()` ë¸”ë¡ ë‚´ì˜ ê³„ì‚°ì— ëŒ€í•´ì„œëŠ” Gradient Trackingì„ í•˜ì§€ ì•ŠëŠ”ë‹¤. ë³´í†µ ëª¨ë¸ í•™ìŠµ í›„ Demo runì„ í•˜ëŠ” ì½”ë“œì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤. (ë‚˜ì¤‘ì— ë‹¤ë£° ì˜ˆì •ì´ë‹·!)

## `nn.Linear`

ì´ë²ˆì—ëŠ” ë‹¤ì°¨ì›ì˜ íšŒê·€ ë¬¸ì œë¥¼ í’€ì–´ë³´ì! `W` í…ì„œë¥¼ `(100, 5)`ì˜ shapeìœ¼ë¡œ ì •ì˜í•´ì„œ ì•ì˜ ê³¼ì •ì„ ê·¸ëŒ€ë¡œ í•´ë„ ë˜ê¸°ëŠ” í•˜ëŠ”ë° ì´ë²ˆì—ëŠ” `nn.Linear()`ë¥¼ ì‚¬ìš©í•´ì„œ íšŒê·€ ë¬¸ì œë¥¼ í’€ì–´ë³´ì.

ì! ì¼ë‹¨ ë‹¤ì°¨ì› ë°ì´í„°ë¶€í„° ì¤€ë¹„í•˜ì. ì´ë²ˆì—ëŠ” GitHubì— ìˆëŠ” ì˜ˆì‹œ ë°ì´í„°ì…‹ [50_Startups.csv](https://github.com/mahesh147/Multiple-Linear-Regression/blob/master/50_Startups.csv)ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì•„ë˜ ëª…ë ¹ì–´ë¡œ ë°ì´í„°ì…‹ì„ colabìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆë‹¤.

```bash
!wget https://raw.githubusercontent.com/mahesh147/Multiple-Linear-Regression/master/50_Startups.csv
```

ê·¸ë¦¬ê³  pandasë¥¼ ì´ìš©í•´ csv íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.

```py
import pandas as pd

data = pd.read_csv('50_Startups.csv')
data.head()
```

X, yë¡œ ë¶„ë¦¬í•˜ì.

```py
X = data[['R&D Spend', 'Administration', 'Marketing Spend']]
y = data[['Profit']]

print("X.shape", X.shape)
print("y.shape", y.shape)
```

ì´ì œ `nn.Linear()`ë¥¼ ì‚¬ìš© í•´ë³´ì. ì•„ë˜ì™€ ê°™ì´ `layer`ë¥¼ ì„ ì–¸í•œë‹¤.

```py
import torch.nn as nn

layer = nn.Linear(in_features=3, out_features=1)
print(layer)
```

`nn.Linear()`ëŠ” `in_features`, `out_features` 2ê°€ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì…ë ¥ ë°›ëŠ”ë‹¤. ë§ê·¸ëŒ€ë¡œ í”¼ì³ ì°¨ì› ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ë©´ ëª‡ê°€ì§€ ì •ë³´ë¥¼ ë” í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°,

```
Linear(in_features=3, out_features=1, bias=True)
```

`nn.Linear()`ì—ì„œëŠ” ì•Œì•„ì„œ bias í…€ì„ ì“°ë„ë¡ ì§€ì •í•  ìˆ˜ ìˆë‹¤. ì•ì—ì„œ `W`, `b`ë¥¼ ì§ì ‘ ì„ ì–¸í•´ì„œ ì¼ë˜ ê²ƒê³¼ ë‹¤ë¥´ê²Œ ì¢€ë” í¸í•˜ê²Œ layerë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ì! ì´ì œ `layer`ë¥¼ ì‚¬ìš©í•´ë³´ì.

```py
X_data = torch.FloatTensor(X.values)
y_data = torch.FloatTensor(y.values)

layer(X_data)
```

ì´ë²ˆì—ëŠ” `torch.FloatTensor()`ë¥¼ ì¼ëŠ”ë° ë°ì´í„° íƒ€ì…ì„ ë§ì¶°ì£¼ë ¤ê³  ì‚¬ìš©í–ˆë‹¤. ë³„ë¡œ ì¤‘ìš”í•˜ì§€ëŠ” ì•Šë‹¤.

ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ë©´ `.grad_fn`ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ ìë™ìœ¼ë¡œ Autogradê°€ ì ìš©ëœ ê²ƒë„ ë³¼ ìˆ˜ ìˆë‹¤.

ì´ì œ GDë¥¼ êµ¬ì¶•í•´ë³´ì.

```py
X_data = torch.FloatTensor(X.values)
y_data = torch.FloatTensor(y.values)
layer = nn.Linear(in_features=3, out_features=1)

for _ in range(10):
  y_pred = layer(X_data)
  loss = ((y_pred - y_data)**2).sum()
  print(loss)

  loss.backward()

  eta = 1e-6
  with torch.no_grad():
    for p in layer.parameters():
        p.sub_(eta * p.grad)
        p.grad.zero_()
```

ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ë©´ loss ê°’ì´ í­ë°œğŸš€í•  ê²ƒì´ë‹¤. ì´ê±´ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ì§€ ì•Šì•„ì„œ ì¸ë° ë¹ ë¥´ê²Œ ì •ê·œí™”í•´ë³´ì.

```py
from sklearn.preprocessing import MinMaxScaler

transformer = MinMaxScaler()
transformer.fit(X)
X = transformer.transform(X)

transformer = MinMaxScaler()
transformer.fit(y)
y = transformer.transform(y)
```

```py
X_data = torch.FloatTensor(X)
y_data = torch.FloatTensor(y)
layer = nn.Linear(in_features=3, out_features=1)

for _ in range(10):
  y_pred = layer(X_data)
  loss = ((y_pred - y_data)**2).sum()
  print(loss)

  loss.backward()

  eta = 1e-2
  with torch.no_grad():
    for p in layer.parameters():
        p.sub_(eta * p.grad)
        p.grad.zero_()
```

ì! ì´ê²ƒìœ¼ë¡œ `nn.Linear()`ë¥¼ ì‚¬ìš©í•´ íšŒê·€ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ë„ ì‚´í´ë´¤ë‹¤. ì´ ë…€ì„ì€ ì•ìœ¼ë¡œ ì •ë§ ìì£¼ ë³¼ ì˜ˆì •ì´ë‹¤! ğŸ‘

<hr/>

[^1]: Transfer Learningì´ë¼ë˜ê°€... fine-tuningì´ë¼ë˜ê°€...