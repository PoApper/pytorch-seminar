---
title: "Seminar 10: Transformer (ì´ë¡ )"
layout: post
use_math: true
tags: ["seminar"]
---

<br/>

ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ì§€í‚¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ppt í˜•íƒœë¡œ ì§„í–‰ë©ë‹ˆë‹¤. ì•„ë˜ì˜ ìˆ˜ì—…ìë£Œë¥¼ ì°¸ì¡°í•´ì£¼ì„¸ìš” ğŸ™

[[material link]](https://github.com/PoApper/pytorch-seminar/blob/main/colab_notebooks/seminar10-Transformer-(1).pdf)

<br/>

ps. seminar10ì€ ë³„ë„ì˜ HWê°€ ì—†ìŠµë‹ˆë‹¤!! Transformerë¥¼ ì´í•´í•˜ëŠ”ë° ìµœì„ ì„ ë‹¤í•´ë´…ì‹œë‹¤!

## references

- [ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/31379)
- [[NLP ë…¼ë¬¸ êµ¬í˜„] pytorchë¡œ êµ¬í˜„í•˜ëŠ” Transformer (Attention is All You Need)](https://cpm0722.github.io/pytorch-implementation/transformer)
- [[Transformer]-1 Positional Encodingì€ ì™œ ê·¸ë ‡ê²Œ ìƒê²¼ì„ê¹Œ? ì´ìœ ](https://velog.io/@gibonki77/DLmathPE)