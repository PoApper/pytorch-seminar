---
title: "Seminar 2"
layout: post
use_math: true
tags: ["seminar"]
---

### í‚¤ì›Œë“œ

- `train_test_split`
- PyTorch `Dataset` and `DataLoader`
- PyTorch `Optimizer`

<hr/>

## Train / Test / Validation

ë³´í†µ ML/DL ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê³¼ì •ì€ í•œ ë²ˆì˜ í•™ìŠµìœ¼ë¡œ ì´ë¤„ì§€ì§€ ì•ŠëŠ”ë‹¤. ML/DLì˜ ëª©í‘œëŠ” ì¡´ì¬í•˜ëŠ” ì†ŒëŸ‰ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì•ìœ¼ë¡œ ë§ˆì£¼í•  ëŒ€ëŸ‰ì˜ **<u>ë¯¸ì§€ì˜ ë°ì´í„°ì…‹ì—ì„œ ë†’ì€ ì„±ëŠ¥ìœ¼ë¡œ ì˜ˆì¸¡</u>**í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ML/DLì€ ë°ì´í„°ì…‹ì„ Train, Test, Validationìœ¼ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.

Train, Test, Validationì„ ì™œ ë‚˜ëˆ„ëŠ”ì§€ ê·¸ë¦¬ê³  ì–´ë–¤ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ”ì§€, K-fold Cross Validation ë“±ë“±ì— ëŒ€í•´ì„œëŠ” ì´ë¯¸ ì•Œê³  ìˆì„ ê±°ë¼ê³  ìƒê°í•˜ê³  ë”°ë¡œ ì„¤ëª…í•˜ì§€ ì•Šê² ë‹¤. í˜¹ì‹œ ëª¨ë¥¸ë‹¤ë©´ ì•„ë˜ì˜ ì•„í‹°í´ë“¤ì„ ì°¸ê³ í•˜ë¼.

- [About Train, Validation and Test Sets in Machine Learning](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)
- [ë°ì´í„°ì…‹ì˜ ë¶„ë¦¬-train, test, validation](https://sevillabk.github.io/3-dateset-split/)

ì¼ë‹¨ ì €ë²ˆ ì„¸ë¯¸ë‚˜ì—ì„œ `nn.Linear()`ë¥¼ ì‚¬ìš©í•´ êµ¬ì¶•í–ˆë˜ Linear Regression ì½”ë“œì—ì„œ ì‹œì‘í•˜ì.

```
!wget https://raw.githubusercontent.com/mahesh147/Multiple-Linear-Regression/master/50_Startups.csv
```

```py
import torch
import torch.nn as nn
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

data = pd.read_csv('50_Startups.csv')
X = data[['R&D Spend', 'Administration', 'Marketing Spend']]
y = data[['Profit']]

transformer = MinMaxScaler()
transformer.fit(X)
X = transformer.transform(X)

transformer = MinMaxScaler()
transformer.fit(y)
y = transformer.transform(y)

X_data = torch.FloatTensor(X)
y_data = torch.FloatTensor(y)
layer = nn.Linear(in_features=3, out_features=1)

for _ in range(10):
  y_pred = layer(X_data)
  loss = ((y_pred - y_data)**2).sum()
  print(loss)

  loss.backward()

  eta = 1e-2
  with torch.no_grad():
    for p in layer.parameters():
        p.sub_(eta * p.grad)
        p.grad.zero_()
```

ìš°ë¦¬ëŠ” skicit-learnì˜ `train_test_split()` í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë°ì´í„°ì…‹ì„ ë¶„ë¦¬í•  ê²ƒì´ë‹¤. ì½”ë“œë¥¼ ë¨¼ì € ì‚´í´ë³´ì.

```py
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)

print("train:", len(X_train))
print("val:", len(X_val))
print("test:", len(X_test))
```

ë³´í†µ Train/Testë¥¼ 0.8/0.2 ì •ë„ë¡œ ë‚˜ëˆ„ê³ , Train/Valë„ 0.8/0.2 ì •ë„ë¡œ ë‚˜ëˆˆë‹¤. `train_test_split()` í•¨ìˆ˜ ìì²´ë¥¼ ì™¸ìš¸ í•„ìš˜ ì—†ê³ , train setì„ ë¶„ë¦¬í•˜ë ¤ê³  í•  ë•Œ `train_test_split()`ë¥¼ ì“´ë‹¤ ì •ë„ë§Œ ê¸°ì–µí•˜ë©´ ëœë‹¤.

ì´ì œ Train/Val/Testë¥¼ ì“°ë„ë¡ ì½”ë“œë¥¼ ìˆ˜ì •í•´ë³´ì.

```py
transformer = MinMaxScaler()
transformer.fit(X_train)
X_train = transformer.transform(X_train)
X_val = transformer.transform(X_val)
X_test = transformer.transform(X_test)

transformer = MinMaxScaler()
transformer.fit(y_train)
y_train = transformer.transform(y_train)
y_val = transformer.transform(y_val)
y_test = transformer.transform(y_test)

X_train = torch.FloatTensor(X_train)
X_val = torch.FloatTensor(X_val)
X_test = torch.FloatTensor(X_test)

y_train = torch.FloatTensor(y_train)
y_val = torch.FloatTensor(y_val)
y_test = torch.FloatTensor(y_test)

layer = nn.Linear(in_features=3, out_features=1)

for i in range(101):
  y_pred = layer(X_train)
  loss = ((y_pred - y_train)**2).sum()

  loss.backward()

  if i % 10 == 0:
    print(f"===== iter: {i} =====")
    print(f"[train]: {loss:.2f}")

    with torch.no_grad():
      y_pred = layer(X_val)
      loss = ((y_pred - y_val)**2).sum()

      print(f"[val]: {loss:.2f}")

  eta = 1e-2
  with torch.no_grad():
    for p in layer.parameters():
        p.sub_(eta * p.grad)
        p.grad.zero_()


y_pred = layer(X_test)
loss = ((y_pred - y_test)**2).sum()

print(f"===== final test loss =====")
print(f"[test]: {loss:.2f}")
```

train/val/test ë°ì´í„°ì…‹ì— `MinMaxScaler()`, `FloatTensor()` ë“±ë“±ì˜ ì‘ì—…ì„ ë‹¤ í•´ì£¼ë©´ ëœë‹¤. ì½”ë“œê°€ ì§€ì €ë¶„í•œë° ì›ë˜ëŠ” ì´ë ‡ê²Œ í•˜ë©´ ì•ˆ ë˜ê³ , ì ì ˆíˆ ëª¨ë“ˆí™” í•´ì„œ ì½”ë“œë¥¼ ì¬ì‚¬ìš© í•´ì•¼ í•œë‹¤.

í•™ìŠµëœ ê²°ê³¼ë¥¼ ì‚´í´ë³´ì. ë³¸ì¸ ë…¸íŠ¸ë¶ì—ì„œì˜ ê²°ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¦¬í¬íŠ¸ í•˜ê² ë‹¤.

```text
===== fianl loss ====
[train]: 0.06
[val]: 0.07
[test]: 0.03
```

ê²°ê³¼ë¥¼ ë³´ë©´ train/val/test ëª¨ë‘ 0ì— ê·¼ì ‘í•œ loss ê°’ì„ ê°€ì§„ë‹¤. ì´ ì •ë„ë©´ ì˜ í•™ìŠµ ë˜ì—ˆë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤. train/val/testì˜ ê²°ê³¼ê°’ì„ í•´ì„í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì„ ë””ìì¸ í•˜ëŠ” ê²ƒ ë§Œí¼ ì¤‘ìš”í•˜ë‹¤. ìœ„ì˜ ì½”ë“œì—ì„œëŠ” ê°„ë‹¨í•œ linear regressionì„ ì‚¬ìš©í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë“  trainì—ì„œ converge í•œë‹¤ê³  val/testì—ì„œë„ converge í•  ê²ƒì„ì„ ë³´ì¥í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. <span class="half_HL">ì–´ë–¤ ëª¨ë¸ì€ trainì—ì„œëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ val/testì—ì„œëŠ” ì¢‹ì§€ ëª»í•œ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ë„ ìˆëŠ” ê²ƒì´ë‹¤.</span> ê·¸ëŸ¬ë‚˜ ì´ë²ˆ ê²°ê³¼ì—ì„œëŠ” train/val/test ëª¨ë‘ 0ì— ê°€ê¹Œìš´ ê·¸ë¦¬ê³  lossê°€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ converge í–ˆë‹¤. ì´ëŠ” linear regressionìœ¼ë¡œ ëª¨ë¸ë§ í•˜ëŠ”ê²Œ ê½¤ ê·¸ëŸ´ë“¯ í•˜ë‹¤ëŠ” í•´ì„ì„ ê°€ëŠ¥ì¼€ í•œë‹¤.

<hr/>

## Dataset and DataLoader

ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ Batch GDì˜ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí–ˆë‹¤. ê·¸ëŸ¬ë‚˜ Pre HW2-1ì—ì„œë„ ë‹¤ë¤˜ë“¯ì´ Batch GD ë³´ë‹¤ëŠ” Stochastic GDê°€ ë” ì„ í˜¸ëœë‹¤. SGD í•™ìŠµ ë°©ì‹ì„ ì§ì ‘ ì½”ë“œë¡œ êµ¬í˜„í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ, PyTorchì—ì„œëŠ” SGDì— ëŒ€í•œ ê¸°ëŠ¥ë„ ì œê³µí•œë‹¤. PyTorchë¡œ SGDë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ PyTorchì˜ `Dataset`, `DataLoader`ì— ëŒ€í•´ ì‚´í´ë³´ì.

```py
from torch.utils.data import Dataset
```

ë¨¼ì € ìš°ë¦¬ëŠ” PyTorch `Dataset`ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤. `Dataset`ì€ abstract classë¡œ PyTorchì—ì„œì˜ ë°ì´í„°ì…‹ì„ í‘œí˜„í•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤. (ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” pandas.DataFrameì´ë‚˜ np.arrayë¡œ ë°ì´í„°ì…‹ì„ í‘œí˜„í–ˆë‹¤.)

ì¼ë‹¨ ì•„ë˜ì™€ ê°™ì´ `Dataset`ì„ ìƒì† ë°›ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì.

```py
from torch.utils.data import Dataset

class MyDataset(Dataset):
  def __init__(self, df):
    self.dataset = df
```

PyTorch `Dataset`ì„ ë§Œë“¤ ë•ŒëŠ” `__len__()`, `__getitem__()` ì´ ë‘ ê°€ì§€ ë©”ì†Œë“œë¥¼ override í•´ì¤˜ì•¼ í•œë‹¤. `__len__()`ì€ `len(dataset)`ì„ í–ˆì„ ë•Œ ë°ì´í„°ì…‹ì˜ ì‚¬ì´ì¦ˆë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ê³ , `__getitem__()`ì€ `dataset[i]`ì™€ ê°™ì´ indexë¡œ ë°ì´í„°ì…‹ì— ì ‘ê·¼í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.

```py
class MyDataset(Dataset):
  def __init__(self, df):
    self.dataset = df

  def __len__(self):
    return len(self.dataset)

  def __getitem__(self, idx):
    return self.dataset.loc[idx]
```

ì¼ë‹¨ì€ ì—¬ê¸°ê¹Œì§€ êµ¬í˜„í•˜ê³  ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ì–´ë³´ì.

```py
dataset = MyDataset(data)

print(len(dataset))
print(dataset[0])
```

ì½”ë“œë¥¼ ë‹¤ë“¬ì–´ì„œ `__getitem__()`ì´ X, yì˜ pairë¥¼ ë°˜í™˜í•˜ë„ë¡ ë°”ê¿”ë³´ì.

```py
class MyDataset(Dataset):
  def __init__(self, df):
    self.X = df[['R&D Spend', 'Administration', 'Marketing Spend']]
    self.y = df[['Profit']]

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return (self.X.loc[idx], self.y.loc[idx]) # ì²˜ìŒì—ëŠ” ì´ê±¸ë¡œ í•˜ê³ , ë’¤ì—ì„œëŠ” ì•„ë˜ì˜ ì½”ë“œë¡œ ë°”ê¾¸ì.
    # return (torch.FloatTensor(self.X[idx]), torch.FloatTensor([self.y[idx]]))


dataset = MyDataset(data)

print(len(dataset))
print(dataset[0])
```

PyTorch Datasetì€ ì´ë ‡ê²Œ `(X, y)`ì˜ pairë¡œ ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ ë˜ëŠ” `{"X": X, "y": y}`ì™€ ê°™ì´ map í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” ê²Œ ì¼ë°˜ì ì´ë‹¤.

ì´ë²ˆì—ëŠ” `MyDataset`ë¥¼ ìƒì„±í•  ë•Œ pre-processingì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ `MinMaxScaler`ë¥¼ ë„˜ê²¨ì£¼ì. constructorì™€ ì½”ë“œë¥¼ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•œë‹¤.

```py
data = data[['R&D Spend', 'Administration', 'Marketing Spend', 'Profit']]
data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)
data_train, data_val = train_test_split(data_train, test_size=0.2, random_state=1)

scaler = MinMaxScaler()
scaler.fit(data_train)

class MyDataset(Dataset):
  def __init__(self, df, scaler):
    df = scaler.transform(df)
    self.X = df[:, :3]
    self.y = df[:, -1]

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return (self.X[idx], self.y[idx])

dataset = MyDataset(data, scaler)

print(len(dataset))
print(dataset[0])
```

ì´ë ‡ê²Œ í•˜ë©´ train/val/testë¥¼ ìš´ìš©í•˜ëŠ” ê²ƒë„ í•œê²° ì‰¬ì›Œì§„ë‹¤.

```py
dataset_train = MyDataset(df_train, scaler)
dataset_val = MyDataset(df_val, scaler)
dataset_test = MyDataset(df_test, scaler)
```

ì§€ê¸ˆì€ ì •ë§ ê°„ë‹¨í•œ ìˆ˜ì¤€ì˜ `Dataset`ì„ êµ¬í˜„í–ˆì§€ë§Œ ì¶”í›„ì— ì»´í“¨í„° ë¹„ì „ ì±•í„°ì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê±°ë‚˜ NLP ì±•í„°ì—ì„œ ë‹¨ì–´ ë°ì´í„°ì…‹ì„ ë§Œë“¤ ë•ŒëŠ” ì§€ê¸ˆì˜ ì½”ë“œ ë³´ë‹¤ ì‹ ê²½ì¨ì•¼ í•  ë¶€ë¶„ì´ ë” ë§ì•„ì§„ë‹¤. ğŸ‘

<br/>

ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ PyTorchì˜ `Dataset`ì€ `dataset[idx]`ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© ì–»ëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•œë‹¤. ê·¸ë˜ì„œ ì´ ë…€ì„ìœ¼ë¡œ SGDë¥¼ êµ¬í˜„í•˜ë ¤ë©´ `for...`ë¬¸ì„ ì¨ì„œ í•˜ë‚˜ì”© í•™ìŠµì‹œì¼œì•¼ í•  ê²ƒì´ë‹¤. ğŸ¤¦â€â™‚ï¸ ë˜ mini-batch ì—­ì‹œ ì–´ë–»ê²Œ êµ¬í˜„í•´ì•¼ í• ì§€ ë§‰ë§‰í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ PyTorchì˜ `DataLoader`ë¥¼ í•¨ê»˜ ì“´ë‹¤ë©´ ê±±ì •í•  ê²ƒì´ ì—†ë‹¤!

PyTorchì˜ `DataLoader`ëŠ” ì•„ë˜ì™€ ê°™ì´ `Dataset` ê°ì²´ë¡œ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

```py
from torch.utils.data import DataLoader

dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True)
print(dataloader_train)
```

ì¸ìë¡œ `batch_size`, `shuffle`ì„ ë°›ëŠ”ë°, `batch_size`ëŠ” ë§ ê·¸ëŒ€ë¡œ mini-batchì˜ ì‚¬ì´ì¦ˆì´ê³ , `shuffle`ì€ ë°ì´í„°ì…‹ì˜ ìˆœì„œëŒ€ë¡œ mini-batchë¥¼ ì–»ì„ ê²ƒì¸ì§€ ì•„ë‹ˆë©´ shuffleëœ ìˆœì„œë¡œ mini-batchë¥¼ ì–»ì„ ê²ƒì¸ì§€ì— ëŒ€í•œ ì¸ìë‹¤.

`DataLoader`ëŠ” ì¼ì¢…ì˜ generatorì´ê¸° ë•Œë¬¸ì— `for...`ì—ì„œ ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©í•œë‹¤.

```py
for X, y in dataloader_train:
  print(X, y)
```

ì! ì´ì œ ì´ ë…€ì„ì„ ê°€ì§€ê³  ê¸°ì¡´ì˜ ì½”ë“œë¥¼ ìˆ˜ì •í•´ë³´ì.

```py
data = data[['R&D Spend', 'Administration', 'Marketing Spend', 'Profit']]
data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)
data_train, data_val = train_test_split(data_train, test_size=0.2, random_state=1)

scaler = MinMaxScaler()
scaler.fit(data_train)

dataset_train = MyDataset(df_train, scaler)
dataset_val = MyDataset(df_val, scaler)
dataset_test = MyDataset(df_test, scaler)

dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True)

layer = nn.Linear(in_features=3, out_features=1)

MAX_EPOCH = 101
for epoch in range(MAX_EPOCH):
  # SGD
  for X, y in dataloader_train:
    y_pred = layer(X)
    loss = ((y_pred - y)**2).sum()
    loss.backward()

    eta = 1e-2
    with torch.no_grad():
      for p in layer.parameters():
          p.sub_(eta * p.grad)
          p.grad.zero_()
```

ì•„ì§ì€ `DataLoader`ë„ train ë°–ì— ì•ˆ ë§Œë“¤ì—ˆë‹¤. ë‹¤ìŒì€ loss logging ë¶€ë¶„ì„ êµ¬í˜„í•œë‹¤. ì´ë¥¼ ìœ„í•´ `evaluate()` í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì.

```py
def evaluate(dataloader):
  total_loss = 0

  with torch.no_grad():
    for X, y in dataloader:
      y_pred = layer(X)
      loss = ((y_pred - y)**2).sum()
      total_loss += loss
  
  print(f'{total_loss:.2f}')
```

ì´ì œ ê¸°ì¡´ ì½”ë“œì— `evaluate()` í•¨ìˆ˜ë¥¼ ë¼ì›Œë„£ìœ¼ë©´ ëœë‹¤.

```py
MAX_EPOCH = 101
for epoch in range(MAX_EPOCH):
  # SGD
  ...
  
  # Evaluation
  if epoch % 10 == 0:
    print(f"===== epoch: {epoch} =====")
    print("[train]")
    evaluate(dataloader_train)
```

ì´ì œ val/testì— ëŒ€í•œ dataloaderë¥¼ ì‘ì„±í•˜ë©´ ë˜ëŠ”ë°, `batch_size`ëŠ” ì ë‹¹íˆ ì„¤ì •í•˜ê³ , `shuffle`ì„ êº¼ì£¼ë©´ ëœë‹¤.

```py
...
dataloader_val = DataLoader(dataset_val, batch_size=4, shuffle=False)
dataloader_test = DataLoader(dataset_test, batch_size=4, shuffle=False)
...

for epoch in range(MAX_EPOCH):
  # SGD
  ...

  # Evaluation
  if epoch % 10 == 0:
    print(f"===== epoch: {epoch} =====")
    print("[train]")
    evaluate(dataloader_train)
    print("[val]")
    evaluate(dataloader_val)
    print("[test]")
    evaluate(dataloader_test)
```

<hr/>

## Optimizer

ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” Batch GDì™€ SGDë¥¼ ì§ì ‘ êµ¬í˜„í–ˆë‹¤.

```py
for X, y in dataloader_train:
  y_pred = layer(X)
  loss = ((y_pred - y)**2).sum()
  loss.backward()

  eta = 1e-2
  with torch.no_grad():
    for p in layer.parameters():
        p.sub_(eta * p.grad)
        p.grad.zero_()
```

ê·¸ëŸ¬ë‚˜! PyTorchì—ì„œëŠ” ì´ë¯¸ SGDë¥¼ êµ¬í˜„ í•´ë‘” ëª¨ë“ˆì´ ìˆë‹¤! ê·¸ê²ƒì´ ë°”ë¡œ PyTorch `Optimizer`ë‹¤.

```py
import torch.optim as optim

optimizer = optim.SGD(layer.parameters(), lr=0.01)
```

optimizerëŠ” *parameter*ì™€ lr, momentum ë“±ë“±ì˜ hyper-parameterë¥¼ ì¸ìë¡œ ë°›ëŠ”ë‹¤. ì´ë•Œ, *parameter*ë€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ í•™ìŠµì˜ ëŒ€ìƒì´ ë˜ëŠ” ë…€ì„ìœ¼ë¡œ weight, biasë¥¼ ìƒê°í•˜ë©´ ëœë‹¤.

ì´ì œ ì´ ë…€ì„ì„ í™œìš©í•´ ê¸°ì¡´ ì½”ë“œë¥¼ ìˆ˜ì •í•´ë³´ì.

```py
layer = nn.Linear(in_features=3, out_features=1)
optimizer = optim.SGD(layer.parameters(), lr=0.01)

MAX_EPOCH = 101
for epoch in range(MAX_EPOCH):
  # SGD
  for X, y in dataloader_train:
    y_pred = layer(X)
    loss = ((y_pred - y)**2).sum()

    # Back-prop
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  
  # Evaluation
  ...
```

ì™€ìš°! GDë¥¼ êµ¬í˜„í•œ ë¶€ë¶„ì´ `optimizer.step()` í•œì¤„ë¡œ ë°”ë€Œì—ˆë‹¤!! ğŸ‘ ì´ì „ê³¼ ë¹„êµí•´ `loss.backward()` ì „í›„ë¡œ `optimzer`ì˜ ì½”ë“œê°€ ë“¤ì–´ì™”ëŠ”ë°, `optimizer.zero_grad()`ëŠ” optimizerì— ë“±ë¡ëœ ëª¨ë¸ì˜ parameterì˜ gradient ê°’ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™” í•´ì£¼ëŠ” í•¨ìˆ˜ê³ , `optimizer.step()`ì€ `backward()` ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ `grad`ë¥¼ parameterì— ì ìš©í•˜ëŠ” í•¨ìˆ˜ë‹¤. 

PyTorch `Optimizer`ì—ëŠ” SGD ë§ê³ ë„ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ìµœì í™” Iterative í•œ ìµœì í™” ë°©ë²•ë“¤ì´ êµ¬í˜„ë˜ì–´ ìˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [torch.optim](https://pytorch.org/docs/stable/optim.html) ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ, ë³´í†µì€ `optim.SGD()` ë˜ëŠ” `optim.Adam()`ì„ Optimizerë¡œ ì‚¬ìš©í•œë‹¤ âœ¨

<hr/>

## ë§ºìŒë§

ì˜¤ëŠ˜ ì§„í–‰í•œ ë‚´ìš©ê¹Œì§€ ì˜ ì´í•´í–ˆë‹¤ë©´ PyTorchë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ì¤€ë¹„ëŠ” ëœ ê²ƒì´ë‹¤! ğŸ™Œ ë³´í†µìœ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì€

``` py
# prepare dataset
data = df.read_csv(...)

# create custom dataset class
class MyDataset(Dataset):
  ...

# build DL model
model = nn.Linear(...)

# prepare dataloader
dl_train = DataLoader(...)

# prepare optimizer
optimizer = optim.SGD(...)

# Do SGD
for epoch in range(MAX_EPOCH):
  # train phase
  for X, y in dl_train:
    y_pred = model(X)
    loss = ...

    # Back-prop
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  # eval phase
  evaluate(dl_train)
  evaluate(dl_val)
  evaluate(dl_test)
```

ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµì´ ì§„í–‰ëœë‹¤. í•™ìŠµì´ ì™„ë£Œë˜ë©´ `loss` ê°’ ë˜ëŠ” $R^2$ì„ ë°”íƒ•ìœ¼ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , `lr`, `MAX_EPOCH`, DL model, train-test split ë¹„ìœ¨ ë“±ë“±ì„ ì¡°ì •í•˜ë©´ì„œ í•™ìŠµì„ ê³„ì†í•´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì„ ì°¾ëŠ”ë‹¤. ì´ê²ƒì´ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì´ë‹¤. ğŸ‘

<hr/>

## Tip & Tricks

- val/testì™€ ê°™ì´ forward feedë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” batch_sizeë¥¼ train ë•Œë³´ë‹¤ ì¡°ê¸ˆ ë” í¬ê²Œ ì¡ì•„ë„ ëœë‹¤. backward feedê°€ ì—†ì–´ GPU memoryë¥¼ ëœ ì“°ê¸° ë•Œë¬¸. ë‹¨, ì¼€ë°”ì¼€ë‹¤.
- ë”¥ëŸ¬ë‹ì„ ì •ë§ ì œëŒ€ë¡œ ë°°ìš°ê³  ì‹¶ë‹¤ë©´, ë³¸ ì„¸ë¯¸ë‚˜ì™€ í•¨ê»˜ ë‹¤ë¥¸ ê°•ì¢Œë¥¼ ë³‘í–‰í•´ì„œ ë“£ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤.
  - [Stanford - CS229](http://cs229.stanford.edu/syllabus-autumn2018.html)
  - [Stanford - CS231n](http://cs231n.stanford.edu/2017/syllabus.html)
  - [2019 KAIST ë”¥ëŸ¬ë‹ í™€ë¡œì„œê¸°](https://github.com/heartcored98/Standalone-DeepLearning)
  - ë³¸ì¸ì€ ë”¥ëŸ¬ë‹ ì²˜ìŒ ë°°ìš¸ ë•Œ PyTorch Tutorial / CS229 / CS231n 3ê°œë¥¼ ë³‘í–‰í•˜ë©´ì„œ ê³µë¶€í–ˆë‹¤.
- í˜¹ì‹œ ì´ë²ˆ í•™ê¸°ì— ì»´ê³µê³¼ ì¸ê³µì§€ëŠ¥(CSED442)ì´ë‚˜ ML/DL ê´€ë ¨ ê°•ì˜ë¥¼ ë“£ê³  ìˆëŠ” ë¶„??

