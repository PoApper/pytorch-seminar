---
title: "Homework 9"
layout: post
use_math: true
tags: ["homework"]
---

- due date: 2022-02-06 23:59 제출
  - pdf 또는 png 형식으로 제출할 것
  - 태블릿 또는 **스캔본**으로 제출할 것

<br/>

# P1. seq2seq의 수학적 모델

인코더-디코더 구조의 seq2seq 모델을 수학적으로 해석하면, "conditional language model"이라고 말할 수 있습니다.

<div class="statement" markdown="1">

The sequence-to-sequence model is an example of a Conditional Language Model
- Language Model because the decoder is predicting the
next word of the target sentence y
- Conditional because its predictions are also conditioned on the source sentence x

</div>

대충 "입력 문장이 주어 졌을 때, 출력 문장으로 등장할 확률이 가장 높은 것을 출력하는, 조건부 확률 모델이다."라고 말할 수 있습니다.

Standford의 CS224n 강의에서 해당 부분을 공부하고, 정리해서 제출합시다!

[CS224n: Machine Translation, Attention, Subword Models](http://web.stanford.edu/class/cs224n/#:~:text=Machine%20Translation%2C%20Attention%2C%20Subword%20Models)

# P2. (심화, optional) 바나다우 어텐션

이번 세미나에서 살펴본 Attention은 내적을 사용하는 루옹(Luong) 어텐션 입니다. 바나다우 Attention은 루옹 어텐션을 일반화한 방법입니다.

아래 아티클을 읽고, 바나다우 어텐션을 직접 구현해봅시다.

[딥러닝을 이용한 자연어 처리 입문: 바나다우 어텐션](https://wikidocs.net/73161)

\* colab 노트북을 `.ipynb`로 export해 제출합니다. 

\** 선택 과제 입니다. 시간이 되시는 분은 한번 도전해보세요!
