---
title: "Seminar 5: ResNet"
layout: post
use_math: true
tags: ["seminar"]
---

<br/>

ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” [KellerJordan/ResNet-PyTorch-CIFAR10](https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10)ì™€ [kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar)ì˜ ì½”ë“œë¥¼ í™œìš©í•´ ì¬êµ¬ì„± í–ˆìŒì„ ë¯¸ë¦¬ ë°í™ë‹ˆë‹¤ ğŸ™

ì €ë²ˆ ì„¸ë¯¸ë‚˜ì—ì„œ ResNetì„ í¬í•¨í•œ CNN Architectureì— ëŒ€í•´ ì‚´í´ë³´ì•˜ë‹¤. ì´ë²ˆì—ëŠ” ResNetì„ ì§ì ‘ PyTorch ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì! ğŸ™Œ

<hr/>

## MNIST CNN

```py
class MNISTCNN(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(1, 6, 5)
    self.conv2 = nn.Conv2d(6, 16, 5)
    self.fc1 = nn.Linear(16 * 4 * 4, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)

  def forward(self, x):
    x = F.relu(self.conv1(x))
    x = F.max_pool2d(x, 2)
    x = F.relu(self.conv2(x))
    x = F.max_pool2d(x, 2)

    x = torch.flatten(x, 1)
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return x
```

ìš°ë¦¬ëŠ” MNIST ë°ì´í„°ì…‹ìœ¼ë¡œ ìˆ«ìë¥¼ ë¶„ë¥˜í•˜ëŠ” CNN ëª¨ë¸ê¹Œì§€ êµ¬í˜„í•´ë´¤ë‹¤. ë˜ [HW3]({{"/2021/11/21/homework-3.html" | relative_url}})ë¡œ CIFIAR10 ë°ì´í„°ì…‹ì„ ì“°ëŠ” Image Classifierë¥¼ êµ¬í˜„í–ˆë‹¤. ì´ë²ˆì— êµ¬í˜„í•˜ëŠ” ResNetì€ **CIFIAR100**ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

## ResBlock Overview

![](https://poapper.github.io/pytorch-seminar/images/cnn-architecture-18.png)

ì œì¼ ë¨¼ì € ResNetì˜ ë‹¨ìœ„ê°€ ë  ResBlockì˜ ì½”ë“œë¥¼ ì‚´í´ë³´ì.

```py
class ResBlock(nn.Module):
  def __init__(self, in_channels, out_channels, stride=1):
    super(ResBlock, self).__init__()

    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.relu1 = nn.ReLU(inplace=True) # same as F.relu()
    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
    self.bn2 = nn.BatchNorm2d(out_channels)
    self.relu2 = nn.ReLU(inplace=True)

  def forward(self, x):
    ...
```

ë¨¼ì € ResBlockì˜ êµ¬ì¡°ì— ë§ê²Œ í•„ìš”í•œ layerë¥¼ ì„ ì–¸í•œë‹¤. ìœ„ì˜ ì‚¬ì§„ì„ ë°”íƒ•ìœ¼ë¡œ 2ê°œì˜ Conv layerì™€ 2ê°œì˜ ReLU, ê·¸ë¦¬ê³  conv ì§í›„ì˜ BNì„ ì„ ì–¸í•œë‹¤.

```py
class ResBlock(nn.Module):
  def __init__(self, in_channels, out_channels, stride=1):
    ...

  def forward(self, x):
    residual = x
    
    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu1(out)
    
    out = self.conv2(out)
    out = self.bn2(out)

    out += residual # residual here!
    out = self.relu2(out)
    return out
```

ResNet ëª¨ë¸ì˜ í•µì‹¬ì¸ Residual flowê°€  

```py
  residual = x
  ...
  out += residual
```

ì˜ í˜•íƒœë¡œ êµ¬í˜„ë˜ì—ˆë‹¤! ì½”ë“œë¡œ ë³´ë©´ `+=` í•˜ë‚˜ë¡œ ì •ë§ ê°„ë‹¨í•˜ê²Œ Residual flowë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤! ğŸ™Œ

## ResNet Overview

![](https://poapper.github.io/pytorch-seminar/images/cnn-architecture-19.png)

ResNet ì½”ë“œì—ì„œ ë°°ìš¸ ì ì€ ì´ `ResBlock`ì˜ Residual Flow ë¿ì´ ì•„ë‹ˆë‹¤. ResNet ì½”ë“œì—ì„œ ê¹Šì€ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ì“°ëŠ” ëª‡ê°€ì§€ íŠ¸ë¦­ë“¤ì„ ë³¼ ìˆ˜ ìˆë‹¤ ğŸ‘€

![](https://media.geeksforgeeks.org/wp-content/uploads/20200424011138/ResNet.PNG)

```py
# ì¼ë‹¨ ResNetì˜ ì „ì²´ ì½”ë“œë¥¼ ë³´ì!
class ResNet(nn.Module):
  def __init__(self, n=7):
    super(ResNet, self).__init__()
    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
    self.norm1 = nn.BatchNorm2d(16)
    self.relu1 = nn.ReLU(inplace=True)
    self.layer1 = self._make_layer(n, in_channels=16, out_channels=16, stride=1)
    self.layer2 = self._make_layer(n, in_channels=16, out_channels=32, stride=2)
    self.layer3 = self._make_layer(n, in_channels=32, out_channels=64, stride=2)
    self.layer4 = self._make_layer(n, in_channels=64, out_channels=128, stride=2)
    self.avgpool = nn.AvgPool2d(8)
    self.linear = nn.Linear(128, 10)

  def _make_layer(self, num_layers, in_channels, out_channels, stride):
    layer_list = [ResBlock(in_channels, out_channels, stride)]
    for _ in range(num_layers):
      layer_list.append(ResBlock(out_channels, out_channels))
    return nn.Sequential(*layer_list)
  
  def forward(self, x):
    x = self.conv1(x)
    x = self.norm1(x)
    x = self.relu1(x)

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    x = self.layer4(x)

    x = self.avgpool(x)
    x = x.view(x.size(0), -1)
    x = self.linear(x)
    return x
```

<br/>

ResNetì€ layerê°€ ë§ìœ¼ë©´ 100ê°œë¥¼ ë„˜ì–´ê°€ê¸° ë•Œë¬¸ì— ê·¸ ë§ì€ layer ì „ë¶€ë¥¼ `__init__()`ì— ì„ ì–¸í•˜ëŠ” ê²Œ í˜ë“¤ë‹¤. ê·¸ë˜ì„œ `_make_layer()` í•¨ìˆ˜ë¡œ ì—°ì†ë˜ëŠ” ëª‡ê°œì˜ layerë¥¼ ë¸”ë¡ìœ¼ë¡œ í•œë²ˆì— ì„ ì–¸í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤.

```py
  def _make_layer(self, num_layers, in_channels, out_channels, stride):
    layer_list = [ResBlock(in_channels, out_channels, stride)]
    for _ in range(num_layers):
      layer_list.append(ResBlock(out_channels, out_channels, stride))
    return nn.Sequential(*layer_list)
```

- ì°¸ê³ ë¡œ pythonì˜ `class`ì—ì„œëŠ” C++/Javaì˜ `private` í‚¤ì›Œë“œê°€ ì—†ë‹¤. ê·¸ëŸ¬ë‚˜ í•¨ìˆ˜ì™€ ë³€ìˆ˜ ì•ì— ì–¸ë”ë°” `_`ë¥¼ ë¶™ì—¬ì£¼ë©´ private ë³€ìˆ˜ë‹¤ ë¼ëŠ” ì•”ë¬µì ì¸ ê·œì¹™ì´ ìˆë‹¤ ğŸ‘ `_make_layer()`ì˜ ì•ì˜ `_`ëŠ” ê·¸ëŸ° ì˜ë¯¸ë‹¤.

- pytorchì—ëŠ” ì—¬ëŸ¬ ê°œì˜ layerë¥¼ ë¬¶ì–´ì£¼ëŠ” `nn.Sequential()`ë¼ëŠ” ê°ì²´ê°€ ìˆë‹¤. `list` íƒ€ì…ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ì´ ë…€ì„ì„ ì‚¬ìš©í•´ í•˜ë‚˜ì˜ layer ë¸”ë¡ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤! [[torch: nn.Sequantial]](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) ì•ìœ¼ë¡œë„ ê½¤ ìì£¼ ì“¸ ë…€ì„ì´ë‹ˆ ìµìˆ™í•´ì§€ë©´ ì¢‹ë‹¤ ğŸ™Œ

- `_make_layer()`ì—ì„œëŠ” `for` ë¬¸ì„ ì´ìš©í•´ ì—°ì†ëœ `ResBlock`ì„ ìƒì„±í•œë‹¤. ëª‡ê°œì˜ `ResBlock`ì„ ì‚¬ìš©í• ì§€ëŠ” `num_layers`ë¥¼ í†µí•´ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤.

- `for` ë¬¸ì—ì„œë„ ì–¸ë”ë°” `_`ê°€ ì‚¬ìš©ë˜ì—ˆë‹¤. ë”°ë¡œ ë³€ìˆ˜ë¥¼ ì§€ì •í•˜ê³  ì‹¶ì§€ ì•Šì„ ë–„ ì´ë ‡ê²Œ ì–¸ë”ë°” `_`ë¥¼ ì“°ê¸°ë„ í•œë‹¤. pythonìœ¼ë¡œ ì½”ë”© í•˜ë‹¤ë³´ë©´ ê½¤ ìì£¼ ì“°ê²Œ ëœë‹¤ ğŸ‘ [python ì–¸ë”ë°” (_) ì‚¬ìš©í•˜ê¸°](https://gomguard.tistory.com/125)

<hr/>

## Let's Serve!

ì! ì—¬ê¸°ê¹Œì§€ ì§œë©´ ResNetì˜ êµ¬í˜„ì€ ëë‚¬ë‹¤!! ~~ë„ˆë¬´ ì‰¬ìš´ë°?~~ `torchvision` ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ í•œë²ˆ í™•ì¸í•´ë³´ì. [[torchvision.datasets]](https://pytorch.org/vision/stable/datasets.html)

### ResNet w/ CIFAR10

[serminar3: MNIST CNN]({{"/2021/11/20/seminar-3.html" | relative_url}}) ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ `torchvision` ë°ì´í„°ì…‹ì„ í™œìš©í•œë‹¤. ì½”ë“œ ë§¥ë½ì€ seminar3ì˜ MNIST í•™ìŠµê³¼ ê±°ì˜ ë¹„ìŠ·í•˜ë‹¤.

```py
USE_CUDA = True

# prepare dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0, 1)])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)

myResNet = ResNet(n=5)
if USE_CUDA:
  myResNet = myResNet.cuda()

# prepare dataLoader
BATCH_SIZE = 128
train_dl = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
test_dl = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)

# prepare optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(myResNet.parameters(), lr=1e-4, momentum=0.9)

# Train Model!
total_tic = time.time()
# Train Model!
MAX_EPOCH = 21
for epoch in range(MAX_EPOCH):
  tic = time.time()

  # train
  total_train_loss = 0
  total_train_correct = 0
  for X, y in train_dl:
    optimizer.zero_grad()
    if USE_CUDA:
      X = X.cuda()
      y = y.cuda()

    y_pred = myResNet(X)

    value, index_pred = torch.max(y_pred.data, 1)
    total_train_correct += (index_pred == y).sum().item()
    
    loss = criterion(y_pred, y)
    total_train_loss += loss.item()

    loss.backward()
    optimizer.step()

  # test
  total_test_loss = 0
  total_test_correct = 0
  with torch.no_grad():
    for X, y in test_dl:
      if USE_CUDA:
        X = X.cuda()
        y = y.cuda()

      y_pred = myResNet(X)

      value, index_pred = torch.max(y_pred.data, 1)
      total_test_correct += (index_pred == y).sum().item()
      
      loss = criterion(y_pred, y)
      total_test_loss += loss.item()

  toc = time.time()
  print(f'===== {epoch} ====')
  print(f'elaps: {toc - tic:.1f} sec')
  print(f'[train] loss: {total_train_loss / len(trainset):.4f}, '
        f'acc: {total_train_correct / len(trainset):.3f}')
  print(f'[test] loss: {total_test_loss / len(testset):.4f}, '
        f'acc: {total_test_correct / len(testset):.3f}')
total_toc = time.time()

print(f'[Total Run]: {total_toc - total_tic:.1f} sec')
```

<br/>

ë°ì´í„°ì…‹ë§Œ `CIFAR10`ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ ë  ê²ƒ ê°™ì§€ë§Œ ì•„ë˜ì˜ ì—ëŸ¬ë¥¼ ì–»ëŠ”ë‹¤. ğŸ˜’

```
The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 3
```

ëŒ€ì¶© í…ì„œ í¬ê¸°ê°€ ì„œë¡œ ë§ì§€ ì•Šì•„ì„œ ì—°ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ì–˜ê¸°ì¸ë°, `ResBlock`ì˜ 

```py
  ...
  def forward(self, x):
    ...
    out += self.projection(residual) # residual here!
```

ìš” ë¶€ë¶„ì´ ë¬¸ì œë‹¤. ë””ë²„ê·¸ë¥¼ í•´ë³´ë©´ ì˜¤ë¥˜ê°€ `in_channels != out_channels`ì¸ ìƒí™©ì—ì„œ ì¼ì–´ë‚œë‹¤. `layer2`, `layer3`ì˜ 16 -> 32, 32 -> 64ì—ì„œ ë§ì´ë‹¤. ì´ ê²½ìš°ì—ëŠ” `residual`ê³¼ `out`ì´ ê°ê° 16 ì±„ë„, 32 ì±„ë„ì´ê¸° ë•Œë¬¸ì— ì—°ì‚°ì´ ë¶ˆê°€ëŠ¥í•œ ê²ƒì´ë‹¤.

<br/>

ì‚¬ì‹¤ ì²˜ìŒì— êµ¬í˜„í•œ `ResBlock`ì€ ì™„ì „í•œ í˜•íƒœê°€ ì•„ë‹ˆë¼ì„œ ëª‡ê°€ì§€ë¥¼ ë” êµ¬í˜„ í•´ì¤˜ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ Identity ë§¤í•‘ì„ í•˜ëŠ” `IdentityPadding`ì´ë¼ëŠ” ì»¤ìŠ¤í…€ ëª¨ë“ˆì„ ë§Œë“¤ì–´ì£¼ì.

```py
class IdentityPadding(nn.Module):
  def __init__(self, in_channels, out_channels, stride):
    super(IdentityPadding, self).__init__()
    self.identity = nn.MaxPool2d(1, stride=stride) # í•´ì„¤ ì°¸ì¡°
    self.num_zero_pads = out_channels - in_channels
  
  def forward(self, x):
    out = F.pad(x, (0, 0, 0, 0, 0, self.num_zero_pads), value=0.0)
    out = self.identity(out)
    return out
```

<br/>

ì´ ëª¨ë“ˆì€ `out_channels`ì´ `in_channels` ë³´ë‹¤ í¬ë‹¤ë©´ ì—¬ë¶„ì˜ ì±„ë„ì„ zeroë¡œ padding í•´ì£¼ëŠ” ë§¤í•‘ layerë‹¤. ì½”ë“œë¥¼ ì‚´í´ë³´ë©´,

```py
self.identity = nn.MaxPool2d(1, stride=stride)
```

ìš” ë¶€ë¶„ì€ ì‚¬ì‹¤ `stride=1`ë¼ë©´ ì´ ë…€ì„ì´ ì—†ì–´ë„ ëœë‹¤. `stride!=1`ì¸ ê²½ìš°ë¥¼ ì»¤ë²„í•˜ê¸° ìœ„í•´ `nn.MaxPool2d(1, stride)`ë¥¼ ì“´ ê²ƒ

```py
  if self.num_zero_pads > 0:
      out = F.pad(x, (0, 0, 0, 0, 0, self.num_zero_pads), value=0.0)
```

ìš” ë¶€ë¶„ì€ `F.pad()` í•¨ìˆ˜ë¥¼ ë¨¼ì € ì•Œì•„ì•¼ í•œë‹¤. [[torch: F.pad()]](https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html) `F.pad()`ëŠ” íŒ¨ë”©í•  ìœ„ì¹˜ë¥¼ `pad`ë¼ëŠ” ì¸ìë¥¼ ê²°ì •í•œë‹¤. ìœ„ì™€ ê°™ì´ 6ê°œì˜ ì¸ìë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤.

```
(padding_left, padding_right, padding_top, padding_bottom, padding_front, padding_back)
= (0, 0, 0, 0, 0, self.num_zero_pads)
```

ê·¸ë˜ì„œ ìš°ë¦¬ì˜ ê²½ìš°ëŠ” ê°€ì¥ ë§ˆì§€ë§‰ ì±„ë„ì„ zero padding í•˜ëŠ” ê²Œ ëœë‹¤.

`F.pad()` ëŒ€ì‹  ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì´ë¥¼ êµ¬í˜„í•  ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤. ì§ì ‘ `self.num_zero_pads` ë§Œí¼ì˜ ì±„ë„ì„ ê°–ëŠ” ë™ì¼í•œ ì‚¬ì´ì¦ˆì˜ í…ì„œë¥¼ ìƒì„±í•œ í›„ `torch.stack()` í•¨ìˆ˜ë¡œ ì§ì ‘ ë¶™ì—¬ë²„ë¦´ ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤. ë‹¨, ì´ë ‡ê²Œ í•  ìˆ˜ë„ ìˆê³ , ì €ë ‡ê²Œ í•  ìˆ˜ë„ ìˆë‹¤ëŠ” ê±°ì§€ ë¬¸ì œì—†ì´ ì˜ ë™ì‘í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ëœë‹¤ ğŸ˜‰

<br/>

`IdentityPadding`ì„ `ResBlock`ì— ì ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

```py
class ResBlock(nn.Module):
  def __init__(self, in_channels, out_channels, stride=1):
    super(ResBlock, self).__init__()
    ...
    self.projection = IdentityPadding(in_channels, out_channels, stride)

  def forward(self, x):
    residual = x
    ...
    out += self.projection(residual) # residual here!
    out = self.relu2(out)
    return out
```

<br/>

ì! ì´ëŒ€ë¡œ í•™ìŠµì„ í•´ë³´ì! í•™ìŠµì€ [KellerJordan/ResNet-PyTorch-CIFAR10](https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10#default-hyperparameters)ì˜ hyper-parameterë¥¼ ê·¸ëŒ€ë¡œ ë”°ëë‹¤ ğŸ™ ~~ì›ë˜ëŠ” parameter searchë¥¼ í•´ì„œ ì§ì ‘ ì°¾ì•„ì•¼ í•œë‹¤~~

- num_of_resBlock = 5
- batch_size = 128
- lr = 0.1
- weight_deacy = 1e-4  <small>(L2 regularization)</small>

```py
# prepare dataLoader
BATCH_SIZE = 128
train_dl = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
test_dl = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)

# prepare optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(myResNet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
```

<br/>

ê·¸ëŸ°ë° ì ê¹! ì‹¤ì œ í•™ìŠµì„ í•˜ê¸° ì „ì— ê¸°ì¡´ ì½”ë“œë¥¼ ì¢€ ë‹¤ë“¬ê³  ê°€ê² ë‹¤ ğŸ‘ ì´ì œëŠ” ëŒ€ì¶© trainê³¼ testê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì–´ë–»ê²Œ ëŒì•„ê°€ëŠ”ì§€ ìµìˆ™í•˜ë‹ˆ ë‘˜ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ê² ë‹¤.

```py
def train_model(train_dl, model, criterion, optimizer):
  total_train_loss = 0
  total_train_correct = 0
  total_train_samples = 0

  for X, y in train_dl:
    optimizer.zero_grad()
    if USE_CUDA:
      X = X.cuda()
      y = y.cuda()

    y_pred = model(X)

    value, index_pred = torch.max(y_pred.data, 1)
    total_train_correct += (index_pred == y).sum().item()
    total_train_samples += index_pred.size(0) # add batch size
    
    loss = criterion(y_pred, y)
    total_train_loss += loss.item()

    loss.backward()
    optimizer.step()

  train_loss = total_train_loss / total_train_samples
  train_acc = total_train_correct / total_train_samples
  return train_loss, train_acc

def test_model(test_dl, model, criterion):
  total_test_loss = 0
  total_test_correct = 0
  total_test_samples = 0
  
  with torch.no_grad():
    for X, y in test_dl:
      if USE_CUDA:
        X = X.cuda()
        y = y.cuda()

      y_pred = model(X)

      value, index_pred = torch.max(y_pred.data, 1)
      total_test_correct += (index_pred == y).sum().item()
      total_test_samples += index_pred.size(0) # add batch size

      loss = criterion(y_pred, y)
      total_test_loss += loss.item()
  
  test_loss = total_test_loss / total_test_samples
  test_acc = total_test_correct / total_test_samples
  return test_loss, test_acc
```

<br/>

ë˜, ì´ì œëŠ” `matplotlib`ìœ¼ë¡œ í”Œë¡¯(plot)ì„ ë§Œë“¤ì–´ í•™ìŠµ ê²°ê³¼ë¥¼ ì‚´í´ë³´ê² ë‹¤.

```py
import matplotlib.pyplot as plt

def plot_acc(train_acc_list, test_acc_list, max_epoch, stride=1):
  epochs = range(0, max_epoch, stride)
  plt.plot(epochs, train_acc_list, label="Train Acc")
  plt.plot(epochs, test_acc_list, label="Test Acc")
  plt.title("Accuracy Graph")
  plt.xlabel("epochs")
  plt.ylabel("Accuracy")
  plt.legend()
  plt.show()
```

<br/>

ì´ì œ ì •ë§ë¡œ í•™ìŠµì‹œì¼œë³´ì!

```py
# Train Model!
total_tic = time.time()
MAX_EPOCH = 51
train_acc_list = []
test_acc_list = []
for epoch in range(MAX_EPOCH):
  tic = time.time()

  # train
  train_loss, train_acc = train_model(train_dl, myResNet, criterion, optimizer)
  train_acc_list.append(train_acc)

  # test
  test_loss, test_acc = test_model(test_dl, myResNet, criterion)
  test_acc_list.append(test_acc)

  if epoch % 5 == 0:
    print(f'===== epoch: {epoch} ====')
    print(f'[train] loss: {train_loss:.4f}, '
          f'acc: {train_acc:.3f}')
    print(f'[test] loss: {test_loss:.4f}, '
          f'acc: {test_acc:.3f}')

    toc = time.time()
    print(f'elaps: {toc - tic:.1f} sec')
total_toc = time.time()

print(f'[Total Run]: {total_toc - total_tic:.1f} sec')
plot_acc(train_acc_list, test_acc_list, MAX_EPOCH, 1)
```

ê²°ê³¼ë¥¼ ë³´ë©´ ë’¤ë¡œ ê°ˆìˆ˜ë¡ train ACCëŠ” ë†’ì•„ì§€ëŠ”ë°, test ACCëŠ” ì •ì²´ë˜ê±°ë‚˜ ì¤„ì–´ë“œëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. overfitting! ì‚¬ì‹¤ ë³¸ë˜ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ í•™ìŠµ ì‹œí‚¬ ë• Crop, Flip, Rotate ë“±ìœ¼ë¡œ **Data Augmentation**í•´ì„œ overfittingì„ ë§‰ëŠ” ë°©ë²•ë„ ìˆë‹¤. ì´ë²ˆ ì½”ë“œì—ì„œëŠ” ë³„ë„ì˜ Augmentationì„ ì•ˆ í–ˆìœ¼ë‹ˆ ì–´ëŠ ì •ë„ì˜ overfittingì´ ìƒê¸¸ ìˆ˜ ìˆë‹¤.


<div class="img-wrapper">
  <img src="{{ "/images/resnet-cifar-10-1.png" | relative_url }}" width="75%">
</div>


### ResNet w/ CIFAR100

ì´ë²ˆ ì„¸ë¯¸ë‚˜ í¬ìŠ¤íŠ¸ê°€ ëŠ¦ì–´ì§€ê²Œ ëœ ì£¼ë²”ì´ë‹¤ ğŸ˜’ ìƒê°ë³´ë‹¤ ë…¼ë¬¸ì˜ í¼í¬ë¨¼ìŠ¤ ë§Œí¼ì„ ì¬í˜„í•˜ëŠ”ê²Œ ì–´ë ¤ì›Œì„œ ë³¸ì¸ì€ ì¤‘ê°„ì— í¬ê¸° í–ˆë‹¤ ğŸ˜¢ ë…¼ë¬¸ ê·¸ëŒ€ë¡œ ì¬í˜„í•˜ëŠ”ê²Œ ì‹¤ë ¥ í–¥ìƒì— ì •ë§ ë„ì›€ì´ ë§ì´ ëœë‹¤. ì»´í“¨í„° ë¹„ì „ ìª½ìœ¼ë¡œ ì§„ë¡œë¥¼ ìƒê°í•˜ê³  ìˆë‹¤ë©´ í•œë²ˆ ì¯¤ `CIFAR100` ë°ì´í„°ì…‹ìœ¼ë¡œ ë…¼ë¬¸ì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ ì¬í˜„í•´ë³´ëŠ” ê±¸ ì¶”ì²œí•œë‹¤ ğŸ‘ 

~~ë‚˜ì¤‘ì— ì¤‘ê°„ í…€ í”„ë¡œì íŠ¸ë¡œ ë‚¼ê¹Œ ê³ ë¯¼ ì¤‘ì´ë‹¤ ğŸ¤”~~

### Refactoring More!

PyTorchì˜ `nn.Model` ëª¨ë¸ì—ëŠ” `model.train()`ê³¼ `model.eval()`ë¼ëŠ” í•¨ìˆ˜ê°€ ìˆë‹¤. ê°ê° ëª¨ë¸ì˜ grad ì˜µì…˜ì„ ì¼œê³  ë„ëŠ” í•¨ìˆ˜ì´ë‹¤. ê·¸ë˜ì„œ `with torch.no_grad()` ë¸”ë¡ ì—†ì´ ì•„ë˜ì™€ ê°™ì´ ì½”ë“œë¥¼ ì§¤ ìˆ˜ ìˆë‹¤!

```py
def train_model(train_dl, model, criterion, optimizer):
  ...
  model.train() # ì´ê²Œ ì¶”ê°€ë˜ì—ˆë‹¤!
  for X, y in train_dl:
    ...

def test_model(eval_dl, model, criterion):
  ...
  model.eval() # with torch.no_grad() ëŒ€ì‹  ì´ê²Œ ì¶”ê°€ë˜ì—ˆë‹¤!
  for X, y in test_dl:
    ...
```

<br/>

ê¸°ì¡´ ResNetì—ëŠ” ì‚¬ì‹¤ ê° ë ˆì´ì–´ ë¸”ë¡ ë§ˆë‹¤ ì¡´ì¬í•˜ëŠ” convì˜ ìˆ˜ê°€ ë‹¤ë¥´ë‹¤. ê·¸ë˜ì„œ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ë©´...

```py
# ì¼ë‹¨ ResNetì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ë³´ì!
class ResNet(nn.Module):
  def __init__(self, num_layers: list):
    super(ResNet, self).__init__()
    ...
    self.layer1 = self._make_layer(num_layers[0], in_channels=16, out_channels=16, stride=1)
    self.layer2 = self._make_layer(num_layers[1], in_channels=16, out_channels=32, stride=2)
    self.layer3 = self._make_layer(num_layers[2], in_channels=32, out_channels=64, stride=2)
    self.layer4 = self._make_layer(num_layers[3], in_channels=64, out_channels=128, stride=2)
    ...
```

<br/>

ë˜, ë§¤ë²ˆ ë ˆì´ì–´ ìˆ˜ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ì§€ ì•Šê³  í•¨ìˆ˜í™” í•´ì„œ ì“°ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì´ `ResNet18`, `ResNet34` ë“±ì„ í•¨ìˆ˜ë¡œ ì •ì˜í•´ì„œ ì“¸ ìˆ˜ ìˆë‹¤.

```py
def resnet18():
    return ResNet([2, 2, 2, 2])

def resnet34():
    return ResNet([3, 4, 6, 3])
```


<hr/>

## ë§ºìŒë§

ì´ë²ˆ ì„¸ë¯¸ë‚˜ì—ì„œëŠ” ResNetì„ ì‹¤ì œ ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì•˜ë‹¤. ë…¼ë¬¸ê³¼ ì„¸ë¯¸ë‚˜ ìë£Œë§Œìœ¼ë¡œëŠ” ì‰½ê²Œ ì´í•´í•˜ê¸° ì–´ë ¤ì› ë˜ ResNetì´ ì½”ë“œ êµ¬í˜„ìœ¼ë¡œëŠ” `out += residual`ë¡œ ì •ë§ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„ë˜ëŠ”ê²Œ ì •ë§ ì•„ë¦„ë‹µë‹¤.

ì‚¬ì‹¤ ì´ë²ˆ ì„¸ë¯¸ë‚˜ì—ì„œ êµ¬í˜„í•œ ResNetì€ ì‹¤ì œ ë…¼ë¬¸ì˜ ê²ƒì„ 100% ì¬í˜„í•œ ê²ƒì´ ì•„ë‹ˆë‹¤. ë ˆì´ì–´ ë¸”ë¡ì˜ ì±„ë„ ìˆ˜ë„ ë‹¤ë¥´ê³ , weight initializationì´ë‚˜ BottleneckBlockë„ êµ¬í˜„í•˜ì§€ ì•Šì•˜ë‹¤!

ë¬¼ë¡  ë…¼ë¬¸ì˜ ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ ì´ë¡ ë„ ì˜ ì´í•´ë˜ê³  ì‹¤ë ¥ë„ ë§ì´ ëŠ˜ì§€ë§Œ, pytorchì—ì„œëŠ” ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ êµ¬í˜„ë˜ì–´ ìˆë‹¤ ğŸ™Œ [[pytorch - RESNET]](https://pytorch.org/hub/pytorch_vision_resnet/) ê·¸ë˜ì„œ ë•Œë¡œëŠ” ì˜ êµ¬í˜„ëœ ëª¨ë¸ì„ ê°€ì ¸ë‹¤ê°€ ì¨ì•¼ í•  ìˆ˜ë„ ìˆë‹¤.

ë‹¤ìŒ ì„¸ë¯¸ë‚˜ë¶€í„° NLPë¥¼ ì£¼ì œë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ í…Œí¬ë‹‰ë“¤ì„ ì‚´í´ë³´ì ğŸ‘