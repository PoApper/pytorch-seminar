---
title: "Seminar 6: Naive Bayes Classifier"
layout: post
use_math: true
tags: ["seminar"]
---

<br/>

ì—¬ëŸ¬ë¶„ì´ [HW5]({{"/2021/12/21/hw-5.html" | relative_url}})ì˜ ê³¼ì œë¥¼ ì—´ì‹¬íˆ í–ˆë‹¤ë©´, ìì—°ì–´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ "í† í°í™”(tokenization)"ì„ ì˜ ì´í•´í–ˆì„ ê²ƒì´ë‹¤ ğŸ‘

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì‹¤ì œ ë°ì´í„°ì—ì„œ í† í°í™”ë¥¼ ì ìš©í•˜ê³ , ê°„ë‹¨í•œ ML ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ê² ë‹¤.

<hr/>

## Spam Classifier

[SNS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)ì—ëŠ” 5ì²œê°œ ì •ë„ì˜ ì´ë©”ì¼ ë°ì´í„°ê°€ ìˆë‹¤. 


```text
     v1                                                 v2
0   ham  Go until jurong point, crazy.. Available only ...
1   ham                      Ok lar... Joking wif u oni...
2  spam  Free entry in 2 a wkly comp to win FA Cup fina...
3   ham  U dun say so early hor... U c already then say...
4   ham  Nah I don't think he goes to usf, he lives aro...
```

ë©”ì¼ ë³¸ë¬¸ê³¼ í•¨ê»˜ ìŠ¤íŒ¸(spam) ë©”ì¼ê³¼ ì¼ë°˜(ham) ë©”ì¼ë¡œ ë ˆì´ë¸”ì´ ìˆëŠ”ë°, ì´ ë°ì´í„°ìœ¼ë¡œ ìŠ¤íŒ¸ì¸ì§€ ì•„ë‹Œì§€ ë¶„ë¥˜í•˜ëŠ” ê°„ë‹¨í•œ ë¶„ë¥˜ê¸°ë¥¼ êµ¬í˜„í•´ë³´ê² ë‹¤.

## Naive Bayes Classifier

ë°ì´í„°ë¥¼ ì„œë¡œ ë‹¤ë¥¸ ë ˆì´ë¸”ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¥¼ í‘¸ëŠ” classifierì—ëŠ” ì—¬ëŸ¬ê°€ì§€ ì¢…ë¥˜ê°€ ìˆë‹¤. 

- Logistic Regression
- KNN Classifier
- Naive Bayes Classifier
- Decision-tree based classifiers...

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ê·¸ ì¤‘ì—ì„œ \<Naive Bayes classifier\>ë¼ëŠ” ë¶„ë¥˜ê¸°ë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³¼ ê²ƒì´ë‹¤.

### ì´ë¡ 

ë¨¼ì € Naive Bayesì˜ ì´ë¡ ì„ í•˜ë‚˜ì”© ì‚´í´ë³´ì.

NBì˜ ëª©í‘œëŠ” $p(c_k \mid x)$ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ í™•ë¥ ì€ ë°ì´í„° $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë ˆì´ë¸” $c_k$ì¼ í™•ë¥ ì„ ë§í•œë‹¤. ì¢€ë” í’€ì–´ì“°ë©´...

- $p(S = T \mid x)$: ë°ì´í„° $x$ê°€ ìŠ¤íŒ¸(spam)ì¼ í™•ë¥ 
- $p(S = F \mid x)$: ë°ì´í„° $x$ê°€ í–„(ham)ì¼ í™•ë¥ 

ê·¸ëŸ°ë° ìœ„ì™€ ê°™ì€ ì¡°ê±´ë¶€ í™•ë¥ ì€ ë² ì´ì¦ˆ ì •ë¦¬(Bayes Theorem)ì— ì˜í•´ ì•„ë˜ì™€ ê°™ì´ í’€ì–´ ì“¸ ìˆ˜ ìˆë‹¤.

$$
p(c_k \mid x) = \frac{p(c_k) \cdot p(x \mid c_k)}{p(x)}
$$

ë¶„ëª¨ í˜•íƒœì—ì„œ ê° í™•ë¥ ì˜ ì˜ë¯¸ë¥¼ ì‚´í´ë³´ë©´

- $p(c_k)$: ë ˆì´ë¸”ì´ $c_k$ì¼ í™•ë¥ 
  - $p(S=T)$: ìŠ¤íŒ¸ ë©”ì¼ì¼ í™•ë¥  = (ìŠ¤íŒ¸ ë©”ì¼ì˜ ìˆ˜) / (ì „ì²´ ë©”ì¼ì˜ ìˆ˜)
- $p(x \mid c_k)$: ë ˆì´ë¸”ì´ $c_k$ì¼ ë•Œ, ë°ì´í„° $x$ê°€ ë“±ì¥í•  í™•ë¥ 
- $p(x)$: ë°ì´í„° $x$ê°€ ë“±ì¥í•  í™•ë¥ 

ì´ë•Œ $p(x)$ëŠ” ìƒê°í•  í•„ìš”ê°€ ì—†ëŠ”ê²Œ, ì •í™•í•œ ê°’ì„ êµ¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ìš°ë¦¬ê°€ ì§€ê¸ˆ ë³´ëŠ” ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ ë¬¸ì œë¼ë©´, ë°ì´í„° $x$ëŠ” `"Go until jurong point, crazy.."`ë¼ëŠ” ë©”ì¼ ë³¸ë¬¸ì— í•´ë‹¹í•˜ëŠ”ë°, ì´ ë©”ì¼ ë³¸ë¬¸ì€ ì•„ì£¼ ê±°ëŒ€í•œ ì½”í¼ìŠ¤ ê³µê°„ ìœ„ì˜ í•œ ì ì— ë¶ˆê³¼í•˜ê¸° ë•Œë¬¸ì— í™•ë¥ ì´ ì •ë§ ë‚®ë‹¤. 

ë¬¼ë¡  í™•ë¥ ì„ ì •ì˜í•˜ëŠ” ì „ì²´ ê³µê°„ì„ ëª…í™•íˆ ì •ì˜í•˜ë©´ ì‹¤ì œ $p(x)$ì˜ ê°’ì„ êµ¬í•  ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì§€ê¸ˆ ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ë¬¸ì œì—ì„œëŠ” $p(x)$ ê°’ì„ ì•„ëŠ” ê²ƒì´ ì „í˜€ ì˜ë¯¸ê°€ ì—†ë‹¤.

ì¼ë‹¨ ìš°ë¦¬ëŠ” $p(c_k \mid x)$ì˜ ê°’ì´ í° ë ˆì´ë¸”ì„ ì •ë‹µìœ¼ë¡œ ì—¬ê¸¸ ê²ƒì´ë‹¤. ê·¸ë˜ì„œ

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k \mid x)
$$

ì„ êµ¬í•˜ê²Œ ë˜ëŠ”ë°, $p(S =T \mid x)$ì—ì„œë‚˜ $p(S = F \mid x)$ì—ì„œë‚˜ $p(x)$ëŠ” ê³µí†µë˜ëŠ” ìƒìˆ˜ì´ê¸° ë•Œë¬¸ì— output $y$ë¥¼ êµ¬í•˜ëŠ”ë° ì „í˜€ ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë˜ì„œ

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(x \mid c_k)
$$

ì—¬ê¸°ê¹Œì§€ê°€ NB classifierì—ì„œ 'Bayes'ë¼ëŠ” ì´ë¦„ì´ ë¶™ëŠ” ì´ìœ ë‹¤!

<br/>

ë‹¤ìŒ ë‹¨ê³„ëŠ” $p(x \mid c_k)$ë¥¼ rawí•œ ë°ì´í„° $x$ ê·¸ëŒ€ë¡œ ì“°ëŠ”ê²Œ ì•„ë‹ˆë¼ feature $(a_1, ..., a_n)$ì˜ í˜•íƒœë¡œ ê¸°ìˆ í•˜ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ë©”ì¼ ë³¸ë¬¸ì—ì„œ ëª‡ê°€ì§€ íŠ¹ì§•ì ì¸ ë¶€ë¶„ë“¤ì„ ì •í•  ìˆ˜ ìˆë‹¤. `'free'`ë¼ëŠ” ë‹¨ì–´ì˜ ìœ ë¬´, `'Call'`ë¼ëŠ” ë‹¨ì–´ì˜ ìœ ë¬´, ë³¸ë¬¸ì˜ ê¸¸ì´, ê°™ì€ ë‹¨ì–´ê°€ ìµœëŒ€ ëª‡ë²ˆ ë°˜ë³µë˜ëŠ”ì§€ ë“±ë“±... Feature Extractionì„ ìˆ˜í–‰í•˜ë©´ raw textê°€ ì•„ë‹ˆë¼ ì •ëŸ‰ì ì¸ featureì˜ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

NBì˜ ê²½ìš°ëŠ” íŠ¹ì • ë‹¨ì–´ê°€ ë“±ì¥ í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ featureë¡œ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ $p(w = \text{free} \mid S = T)$ë¼ê³  í•˜ë©´... ìŠ¤íŒ¸ì¸ ë©”ì¼ ì¤‘ì—ì„œ $\text{free}$ë¼ëŠ” ë‹¨ì–´ê°€ ë“±ì¥í•  í™•ë¥ ì„ ë§í•œë‹¤! ì´ê²ƒì€ 

$$
p(w = \text{free} \mid S = T) = \frac{\text{#. of spam mails contain word 'free'}}{\text{#. of spam mails}}
$$

ë¡œ ì‰½ê²Œ í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆë‹¤! ğŸ‘

<br/>

ê·¸ë˜ì„œ word $w_i$ë¥¼ í”¼ì²˜ë¡œ ì‚¼ì•„ $p(x \mid c_k)$ë¥¼ ë‹¤ì‹œ ì ì–´ë³´ë©´ ...

$$
p(x \mid c_k) = p(a_1, ..., a_n \mid c_k)
$$


<br/>

NBëŠ” \<**naive assumption**\>ì´ë¼ëŠ” ê°€ì •ì„ í•œë‹¤. ì´ê²ƒì€ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="statement" markdown="1" align="center">

"One assumption taken is the <span style="color:red;">strong independence</span> assumptions btw the features."

</div>

ì¦‰, ê° í”¼ì³ê°€ ì„œë¡œ **ë…ë¦½**ì´ë‹¤!ë¥¼ ê°€ì •í•œë‹¤. ì´ê²ƒì€ ê³§ $p(a_1, ..., a_n \mid c_k)$ì— ëŒ€í•´ ì•„ë˜ê°€ ì„±ë¦½í•¨ì„ ë§í•œë‹¤.

$$
p(a_1, ..., a_n \mid c_k)
= p(a_1 \mid c_k) p(a_2 \mid c_k) \cdots p(a_n \mid c_k)
$$

ê·¸ë˜ì„œ ê° í”¼ì²˜ì˜ í™•ë¥ ì„ ë‹¨ìˆœíˆ ê³±í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ joint probability $p(a_1, ..., a_n \mid c_k)$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤! ğŸ‘

ë¬¼ë¡  strong indepence ê°€ì •ì€ ëª¨ë¸ë§ì„ ìœ„í•´ ì ë‹¹í•œ í˜•íƒœë¡œ ê°€ì •í•œ ê²ƒì¼ ë¿ì´ë‹¤. ì‹¤ì œë¡  ê° í”¼ì²˜ê°€ ë…ë¦½ì´ ì•„ë‹ˆë¼ correlate ë˜ì–´ ìˆì„ ìˆ˜ ìˆë‹¤.

<br/>

ì! ê·¸ëŸ¼ ì›ë˜ êµ¬í•˜ë ¤ê³  í–ˆë˜ output $y$ë¥¼ ë‹¤ì‹œ êµ¬í•´ë³´ì.

$$
y = \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(x \mid c_k)
$$

$p(x \mid c_k)$ë¥¼ feature $a_i$ì˜ í˜•íƒœë¡œ ë°”ê¾¸ê³  \<naive assumption\>ì— ì˜í•´ ìœ„ì˜ ì‹ì€ ì•„ë˜ê°€ ëœë‹¤.

$$
\begin{aligned}
y 
&= \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(a_1 \mid c_k) \cdots p(a_n \mid c_k) \\
&= \underset{c_k}{\text{argmax}} \; p(c_k) \cdot \prod_i^n p(a_i \mid c_k)
\end{aligned}
$$

ê²°êµ­ ê° ë ˆì´ë¸” $c_k$ì— ëŒ€í•´

-  $S=T$ì—ì„œì˜ $p(c_k) \cdot \prod_i^n p(a_i \mid c_k)$
-  $S=F$ì—ì„œì˜ $p(c_k) \cdot \prod_i^n p(a_i \mid c_k)$  

ê°’ì„ ë¹„êµí•´ ë” í° ë ˆì´ë¸”ì˜ ê°’ì„ output $y$ë¡œ ë§¤ê¸°ë©´ ë˜ëŠ” ê²ƒì´ë‹¤! ğŸ™Œ

<hr/>

### êµ¬í˜„

ì... ê·¸ëŸ¼ ì´ê±¸ ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì! ì‚¬ì‹¤ êµ¬í˜„ ìì²´ë„ ê·¸ë¦¬ ì–´ë µì§€ ì•Šìœ¼ë‹ˆ ì˜ ë”°ë¼ì™€ë³´ì ğŸ˜‰

ì¼ë‹¨ ìŠ¤íŒ¸ ë°ì´í„°ì…‹ [SNS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)ì„ ì¤€ë¹„í•œë‹¤. 

```bash
!wget https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv
```

ê·¸ë¦¬ê³  ì ë‹¹íˆ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ê³ , train-test splitì„ ìˆ˜í–‰í•˜ë©´...

```py
data = pd.read_csv('spam.csv', encoding='latin1')
print("ìƒ˜í”Œ ìˆ˜", len(data)) # 5,572
data.head()

data = data[['v1', 'v2']] # í•„ìš”í•œ ì»¬ëŸ¼í•œ ì¶”ì¶œ
data.head()

data['v1'] = data['v1'].replace(['ham','spam'],[0,1]) # labelì„ 0, 1ë¡œ ë³€ê²½
data.head()
```

```py
data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)

X_train = data_train['v2']
y_train = data_train['v1']
X_test = data_test['v2']
y_test = data_test['v1']

print('=== í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ ===')
print('ham', str(round(np.sum(y_train == 0) / len(y_train) * 100, 1)) + '%')
print('spam', str(round(np.sum(y_train == 1) / len(y_train) * 100, 1)) + '%')
print('=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ ===')
print('ham', str(round(np.sum(y_test == 0) / len(y_test) * 100, 1)) + '%')
print('spam', str(round(np.sum(y_test == 1) / len(y_test) * 100, 1)) + '%')

...

=== í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ ===
ham 86.4%
spam 13.6%
=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ ===
ham 87.5%
spam 12.5%
```

<br/>

ë©”ì¼ ë³¸ë¬¸ì€ ë„ˆë¬´ rawí•œ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì— ë°”ë¡œ ì“°ê¸° ë³´ë‹¤ëŠ” ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ”ê²Œ ì¢‹ë‹¤. [HW5]({{"/2021/12/21/hw-5.html" | relative_url}}) ê³¼ì œì—ì„œ ë°°ìš´ **í† í°í™”(tokenization)**ì„ í™œìš©í•´ë³´ì!

```py
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
```

``` py
def tokenize(text):
  sentences = sent_tokenize(text)
  tokens = []

  for sentence in sentences:
    # ë‹¨ì–´ í† í°í™”
    words = word_tokenize(sentence)
    for word in words:
      word = word.lower() # ì†Œë¬¸ì ë³€í™˜
      if word in stop_words: continue # ë¶ˆìš©ì–´ ì œê±°
      if len(word) <= 3: continue # ê¸¸ì´ 3 ì´í•˜ ì œê±°
      tokens.append(word)
  return tokens
```

í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ í† í°í™”ì™€ ë‹¨ì–´ í† í°í™”ë¥¼ í•œ í›„, ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  ê¸¸ì´ 3 ì´í•˜ì˜ ë‹¨ì–´ë“¤ì„ ëª¨ë‘ ì œê±°í–ˆë‹¤!

ê·¸ë¦¬ê³  `vocab = {}`ì— ê° í† í°ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì €ì¥í•˜ë©´ ...

```py
vocab = {}
for text in X_train:
  # í† í°í™”
  tokens = tokenize(text)

  for token in tokens:
    if token not in vocab:
      vocab[token] = 0
    vocab[token] += 1

print(vocab)
print(len(vocab))

...

{'sleeping': 10, 'feeling': 15, 'well': 86, 'come': 177, ...
7449
```

ì™€ ê°™ì´ ì¶œë ¥ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° ì´ ëª¨ë“  í† í°ì„ ë‹¤ ì“¸ ê±´ ì•„ë‹ˆê³  ë¹ˆë„ìˆ˜ Top 5ì˜ ë‹¨ì–´ì— ëŒ€í•´ì„œë§Œ feature extractionì„ ì ìš©í•  ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì ë‹¹íˆ ì •ë ¬í•´ `target_token`ì„ ì •í•˜ë©´...

```py
# filter out top 5 tokens
vocab_sorted = sorted(vocab.items(), key=lambda x : x[1], reverse=True)

vocab_size = 5
vocab_sorted = vocab_sorted[:vocab_size]
print(vocab_sorted)
target_tokens = [word for word, _ in vocab_sorted]
print(target_tokens)

...

[('call', 460), ('know', 216), ('free', 212), ('like', 204), ('good', 193)]
['call', 'know', 'free', 'like', 'good']
```

ìœ„ì™€ ê°™ì´ 5ê°œì˜ ë‹¨ì–´ê°€ ì¶”ì¶œë˜ì—ˆë‹¤! 

<br/>

ì´ì œ ì´ 5ê°œ ë‹¨ì–´ë¥¼ featureë¡œ ì‚¼ì•„ì„œ í™•ë¥  $p(a_i \mid S = T)$ì™€ $p(a_i \mid S = F)$ë¥¼ ê°ê° êµ¬í•´ë³´ì.

```py
# derive spam probability
X_train_spam = X_train[y_train == 1]
print('[train] # of spam', len(X_train_spam))

spam_count = {token: 0 for token in target_tokens}

for text in X_train_spam:
  tokens = tokenize(text)

  for target_token in target_tokens:
    if target_token in tokens:
      spam_count[target_token] += 1

print('spam count:', spam_count)

word_spam_prob = {token: spam_count[token] / len(X_train_spam) for token in spam_count}
print("P(w_i | S = T)", word_spam_prob) # P(w_i | S = T)
...
[train] # of spam 608
spam count: {'call': 263, 'know': 17, 'free': 132, 'like': 12, 'good': 10}
P(w_i | S = T) {'call': 0.43256578947368424, 'know': 0.027960526315789474, 'free': 0.21710526315789475, ...
```

hamì˜ ê²½ìš°ë„ ë¹„ìŠ·í•˜ê²Œ êµ¬í•˜ë©´ ëœë‹¤.

<br/>

ì´ì œ $p(c_k)$ì— ëŒ€ì‘í•˜ëŠ” $p(S = T)$, $p(S = F)$ë¥¼ êµ¬í•´ë³´ì.

```py
spam_prob = len(X_train_spam) / len(X_train)
ham_prob = 1 - spam_prob
print("spam_prob", spam_prob) # P(S = T)
print("ham_prob", ham_prob) # P(S = F)
```

<br/>

ì´ì œ ì¤€ë¹„ëŠ” ë‹¤ ë˜ì—ˆë‹¤! ë¯¸ë¦¬ êµ¬í•œ í™•ë¥ ê°’ì„ ì¡°í•©í•´ NBë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³´ì!

```py
# apply naive bayes for train-set
correct_count = 0

for idx, text in X_train.items():
  tokens = tokenize(text)
  y_gt = y_train[idx]

  # spam case
  spam_prob_ = spam_prob
  for token in tokens:
    if token in target_tokens:
      spam_prob_ *= word_spam_prob[token]

  # ham case
  ham_prob_ = ham_prob
  for token in tokens:
    if token in target_tokens:
      ham_prob_ *= word_ham_prob[token]

  y_pred = ham_prob_ < spam_prob_
  correct_count += (y_gt == y_pred)

print(f'{round(correct_count / len(X_train) * 100, 1)}%') 
...
89.7%
```

train-setì— ì ìš©í–ˆì„ ë•Œ, 89.7% ì •ë„ì˜ ì •í™•ë„ë¥¼ ë³´ì˜€ë‹¤. ê·¸ëŸ¬ë‚˜ ë¶„ë¥˜ ë¬¸ì œëŠ” ë‹¨ìˆœíˆ ì •í™•ë„ë§Œìœ¼ë¡œëŠ” ì„±ëŠ¥ì´ ì¢‹ë‹¤ ë‚˜ì˜ë‹¤ë¥¼ íŒë‹¨í•  ìˆ˜ ì—†ë‹¤! ì™œëƒí•˜ë©´ í˜„ì¬ì˜ train/test setì—ì„œ "ì–´ë–¤ ë©”ì¼ì´ ì…ë ¥ë˜ì–´ë„ hamìœ¼ë¡œ ë¶„ë¥˜"í•˜ëŠ” classifierë„ hamì˜ ë¹„ìœ¨ì´ ì••ë„ì ìœ¼ë¡œ ë§ê¸° ë•Œë¬¸ì— 80% ì´ìƒì˜ ì •í™•ë„(accuracy)ë¥¼ ë³´ì´ê¸° ë•Œë¬¸ì´ë‹¤!

ì •í™•ë„(accuracy)ì˜ ì´ëŸ° ë‹¨ìˆœí•¨ ë•Œë¬¸ì— ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ì •í™•ë„ë³´ë‹¤ëŠ” **precision-recall** ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ íŒë‹¨í•œë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/precision-recall-1.png" | relative_url }}" width="80%">
</div>

precision-recall ê°œë…ì€ ìœ„ì˜ 2x2ì˜ í‘œë¡œ ì „ë¶€ ì„¤ëª…ë˜ëŠ” ê°œë…ì´ë‹¤. 

- **precision**: positiveë¼ê³  ì˜ˆì¸¡í•œ ê²ƒì¤‘ ì‹¤ì œë¡œ positiveê°€ ìˆëŠ” ë¹„ìœ¨
- **recall**: ì‹¤ì œ positiveì¸ ê²ƒ ì¤‘ì—ì„œ ì œëŒ€ë¡œ ë§ì¶˜ ë¹„ìœ¨

ì†Œìœ„ ë§í•˜ëŠ” 'ì¢‹ì€ ëª¨ë¸'ì´ ë˜ë ¤ë©´ precisionê³¼ recallì´ ë‘˜ë‹¤ ë†’ì€ ê°’ì„ ê°€ì ¸ì•¼ í•œë‹¤. ë§Œì•½ ìœ„ì™€ ê°™ì´ "ì–´ë–¤ ë©”ì¼ì´ ì…ë ¥ë˜ì–´ë„ hamìœ¼ë¡œ ë¶„ë¥˜"í•˜ëŠ” classifierê°€ ìˆë‹¤ë©´, ê·¸ classifierëŠ” 

- 100ê°œ ìƒ˜í”Œì— 20:80 ë¹„ìœ¨ë¡œ spam:hamì´ ë¶„í¬í•¨
- spamì„ positiveë¡œ ë‘”ë‹¤ [^1]
- ì–´ë–¤ ë©”ì¼ì´ ì…ë ¥ë˜ì–´ë„ hamìœ¼ë¡œ ë¶„ë¥˜: all predicted as negative(ham)
- precision: ëª¨ë‘ hamìœ¼ë¡œ ë¶„ë¥˜í•˜ê¸° ë•Œë¬¸ì— precisionì€ $0/0 = 0$
- recall: ì œëŒ€ë¡œ ë¶„ë¥˜í•œ spamì´ ì—†ê¸° ë•Œë¬¸ì— $0/20 = 0$

ê·¸ë˜ì„œ ì´ ê²½ìš°ëŠ” precision, recall ê°’ì´ 0ì´ê¸° ë•Œë¬¸ì— ì¢‹ì€ ëª¨ë¸ì´ ì•„ë‹ˆë¼ê³  íŒë‹¨í•˜ë©°, ì‹¤ì œë¡œë„ hamìœ¼ë¡œë§Œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì€ ì¢‹ì€ ëª¨ë¸ì´ ì•„ë‹ˆë‹¤ ğŸ‘Š

ì–´ì¨Œë“  ì „í•˜ê³  ì‹¶ì€ ë‚´ìš©ì€ ì •í™•ë„(accuracy) ë¿ ì•„ë‹ˆë¼ precision, recall ê°’ì„ ë´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤! precision, recall ê³µì‹ì„ ì§ì ‘ êµ¬í˜„í•´ë„ ë˜ê² ì§€ë§Œ, `sklearn`ì— ë‹¤ êµ¬í˜„ì´ ë˜ì–´ ìˆì–´ì„œ ê°€ì ¸ë‹¤ ì“°ê¸°ë§Œ í•˜ë©´ ëœë‹¤ ğŸ˜Š

```py
from sklearn import metrics

# apply naive bayes for train-set
correct_count = 0
y_preds = []

for idx, text in X_train.items():
  tokens = tokenize(text)
  y_gt = y_train[idx]

  ...

  y_pred = ham_prob_ < spam_prob_
  correct_count += (y_gt == y_pred)
  y_preds.append(y_pred)

precision = metrics.precision_score(y_train.to_list(), y_preds)
recall = metrics.recall_score(y_train.to_list(), y_preds)
f1_score = metrics.f1_score(y_train.to_list(), y_preds)

print('precision:', round(precision, 3))
print('recall:', round(recall, 3))
print('f1_score:', round(f1_score, 3))
print(f'accuracy: {round(correct_count / len(X_train) * 100, 1)}%')
...
precision: 0.642
recall: 0.551
f1_score: 0.593
accuracy: 89.7%
```

ì¶œë ¥ëœ ê°’ì„ ë³´ë©´ ì •í™•ë„ì™€ëŠ” ê°’ì´ ë‹¤ë¥´ê²Œ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ë˜, 0.5 ìˆ˜ì¤€ì´ë©´ í¼í¬ë¨¼ìŠ¤ê°€ ì¢‹ì€ í¸ì´ ì•„ë‹ˆë‹¤ ğŸ˜¢ ë˜, `f1_score`ë¼ëŠ” ì§€í‘œë„ ì‚¬ìš©í–ˆëŠ”ë°, ê³µì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\text{f1_score} = \frac{2 * (\text{precision} * \text{recall})}{\text{precision} + \text{recall}}
$$

ëŒ€ì¶© precision, recallì„ ì¢…í•©í•œ ì§€í‘œ ì •ë„ë¡œ ì´í•´í•˜ë©´ ëœë‹¤. `f1_score`ë„ ì§€í‘œë¡œ ìì£¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í•¨ê»˜ ì•Œì•„ë‘ì! 

<br/>

precision, recall, f1_score ê°’ì„ ë³´ë©´ ì•„ì§ 0.5 ìˆ˜ì¤€ìœ¼ë¡œ ê·¸ë ‡ê²Œ ë†’ì§€ ì•Šë‹¤. ê·¸ë˜ì„œ featureë¡œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ì˜ ìˆ˜ë¥¼ 10ê°œ, 20ê°œ, 100ê°œë¡œ ëŠ˜ë ¸ì„ ë•Œ classifierì˜ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‚´í´ë³´ì! vocab_sizeë¥¼ ëŠ˜ë¦¬ê¸° í¸í•˜ê²Œ ì½”ë“œë¥¼ í•¨ìˆ˜í™” í•˜ë©´ ...

```py
def generate_prob(X, y, vocab_size):
  vocab_sorted = sorted(vocab.items(), key=lambda x : x[1], reverse=True)
  vocab_sorted = vocab_sorted[:vocab_size]
  target_tokens = [word for word, _ in vocab_sorted]

  # derive spam probability
  X_spam = X[y == 1].copy()
  spam_count = {token: 0 for token in target_tokens}

  for text in X_spam:
    tokens = tokenize(text)
    tokens = set(tokens)

    for target_token in target_tokens:
      if target_token in tokens:
        spam_count[target_token] += 1

  word_spam_prob = {token: spam_count[token] / len(X_spam) for token in spam_count}

  X_ham = X[y == 0]
  ham_count = {token: 0 for token in target_tokens}

  for text in X_ham:
    tokens = tokenize(text)
    tokens = set(tokens)

    for target_token in target_tokens:
      if target_token in tokens:
        ham_count[target_token] += 1

  word_ham_prob = {token: ham_count[token] / len(X_ham) for token in ham_count}

  return word_spam_prob, word_ham_prob
```

```py
def evaludate_classifier(X, y, vocab_size=5):
  word_spam_prob, word_ham_prod = generate_prob(X, y, vocab_size)

  correct_count = 0
  y_preds = []

  for idx, text in X.items():
    tokens = tokenize(text)
    y_gt = y[idx]

    # spam case
    spam_prob_ = np.log(spam_prob) # handle unserflow
    for token in tokens:
      if token in word_spam_prob and word_spam_prob[token] != 0:
        spam_prob_ += np.log(word_spam_prob[token])

    # ham case
    ham_prob_ = np.log(ham_prob)
    for token in tokens:
      if token in word_ham_prob and word_ham_prob[token] != 0:
        ham_prob_ += np.log(word_ham_prob[token])

    y_pred = ham_prob_ < spam_prob_
    correct_count += (y_gt == y_pred)
    y_preds.append(y_pred)

  accuracy = correct_count / len(X)
  precision = metrics.precision_score(y.to_list(), y_preds)
  recall = metrics.recall_score(y.to_list(), y_preds)
  f1_score = metrics.f1_score(y.to_list(), y_preds)

  return (accuracy, precision, recall, f1_score)
```

ì´ë•Œ, `evaludate_classifier()`ì—ì„œ `spam_prob_`ì™€ `ham_prob_`ë¥¼ êµ¬í•˜ëŠ” ë¶€ë¶„ì´ í•™ë¥ ì„ ê³±í•˜ëŠ” ê²ƒ ëŒ€ì‹  `np.log()` ê°’ì„ ë”í•˜ëŠ” ê²ƒìœ¼ë¡œ ë°”ë€Œì—ˆë‹¤. ì´ê²ƒì€ í™•ë¥  ê³±ì…ˆì´ ê³„ì†ë˜ì–´ì„œ **ì–¸ë” í”Œë¡œìš°**ê°€ ë°œìƒí•˜ëŠ” ê±¸ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ ìˆ˜ì •í•œ ê²ƒì´ë‹¤!

ì´ì œ ìœ„ì˜ ì½”ë“œë¡œ `vocab_size` 1ë¶€í„° 150ê¹Œì§€ì˜ ëª¨ë¸ ì„±ëŠ¥ì€ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/naive-bayes-accuracy.png" | relative_url }}" width="60%">
</div>

<div class="img-wrapper">
  <img src="{{ "/images/naive-bayes-f1-score.png" | relative_url }}" width="60%">
</div>

ê²°ê³¼ë¥¼ ë³´ë©´ `vocab_size=20`ê¹Œì§€ëŠ” í”¼ì²˜ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ì§€ë§Œ, ê·¸ ì´í›„ë¶€í„°ëŠ” ì„±ëŠ¥ì´ ë‚®ì•„ì§€ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤.

<hr/>

## ë§ºìŒë§

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ìì—°ì–´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³ , ê·¸ê±¸ í•™ìŠµ ë°ì´í„°ë¡œ ì“°ëŠ” Naive Bayes Classifierë¥¼ êµ¬í˜„í•´ë³´ì•˜ë‹¤. NB Classifierê°€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ì•„ë‹ˆì§€ë§Œ, ì›Œë‚™ ìœ ëª…í•œ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— í•œë²ˆì¯¤ ì§ì ‘ êµ¬í˜„í•´ë³´ëŠ” ê±¸ ì¶”ì²œí•œë‹¤.

ë‹¤ìŒ ì„¸ë¯¸ë‚˜ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ RNN, LSTMì„ ì‚´í´ë³´ê³ , ì´ë¥¼ í™œìš©í•œ ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•´ë³´ê² ë‹¤!

<hr/>

[^1]: hamì„ positiveë¼ê³  ë‘˜ ìˆ˜ë„ ìˆê² ì§€ë§Œ, ë³´í†µ ê·¸ ìˆ˜ê°€ ë” ì ì€ ë ˆì´ë¸”ì„ positiveë¡œ ì„¤ì •í•˜ê±°ë‚˜ detect í•´ì•¼ë§Œ í•˜ëŠ” ê²ƒì„ positiveë¡œ ì„¤ì •í•œë‹¤. ì§€ê¸ˆì€ spam detectionì„ ë‹¤ë£¨ì§€ë§Œ, ì•…ì„± ì¢…ì–‘ì„ ë‹¤ë£¨ëŠ” ê²½ìš°ë¼ë©´ ì–‘ì„±(Benign)ê³¼ ì•…ì„±(Malignant) ì¤‘ ì•…ì„±ì¸ ê²½ìš°ë¥¼ positiveë¡œ ì„¤ì •í•œë‹¤.