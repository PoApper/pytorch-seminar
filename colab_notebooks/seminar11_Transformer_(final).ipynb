{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar10: Transformer (final)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "실습 코드는 PyTorch Tutorials의 “[NN.TRANSFORMER와 TORCHTEXT로 언어 번역하기](https://tutorials.pytorch.kr/beginner/translation_transformer.html)” 문서를 참고했음을 밝힙니다."
      ],
      "metadata": {
        "id": "qu8h0QEUfAi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 사전 준비\n",
        "\n",
        "- [spacy tokenizer](https://wikidocs.net/64517)"
      ],
      "metadata": {
        "id": "r2uSCWUDN8Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMDbgSeENslw",
        "outputId": "7b6a690d-dc8e-4ce6-fbc8-a0bd095397e4",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 14.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 42.7 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 73.6 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting de-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 195 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋"
      ],
      "metadata": {
        "id": "3mI1mz9DNSvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "ESYXR9paapN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Iterable, List"
      ],
      "metadata": {
        "id": "ZqiLWD6sNVR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용할 데이터셋 확인\n",
        "## 학습용 데이터 반복자\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "train_iter = Multi30k(root='./data', split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "for idx, data_sample in enumerate(train_iter):\n",
        "  if idx > 10: break\n",
        "  print(idx, data_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KpOVb-NO5C3",
        "outputId": "c5af646e-1d5e-477b-df36-7f92ed6fdc47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.21M/1.21M [00:00<00:00, 4.98MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\\n', 'Two young, White males are outside near many bushes.\\n')\n",
            "1 ('Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\\n', 'Several men in hard hats are operating a giant pulley system.\\n')\n",
            "2 ('Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\\n', 'A little girl climbing into a wooden playhouse.\\n')\n",
            "3 ('Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\\n', 'A man in a blue shirt is standing on a ladder cleaning a window.\\n')\n",
            "4 ('Zwei Männer stehen am Herd und bereiten Essen zu.\\n', 'Two men are at the stove preparing food.\\n')\n",
            "5 ('Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\\n', 'A man in green holds a guitar while the other man observes his shirt.\\n')\n",
            "6 ('Ein Mann lächelt einen ausgestopften Löwen an.\\n', 'A man is smiling at a stuffed lion\\n')\n",
            "7 ('Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\\n', 'A trendy girl talking on her cellphone while gliding slowly down the street.\\n')\n",
            "8 ('Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.\\n', 'A woman with a large purse is walking by a gate.\\n')\n",
            "9 ('Jungen tanzen mitten in der Nacht auf Pfosten.\\n', 'Boys dancing on poles in the middle of the night.\\n')\n",
            "10 ('Eine Ballettklasse mit fünf Mädchen, die nacheinander springen.\\n', 'A ballet class of five girls jumping in sequence.\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer\n",
        "token_transform = {}\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "print(token_transform[SRC_LANGUAGE](\"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "id": "DE7KOxtHQPvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e50d03-9acc-4ef0-f5fb-75e1809b09e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Eine', 'Gruppe', 'von', 'Menschen', 'steht', 'vor', 'einem', 'Iglu', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수 토큰\n",
        "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
        "\n",
        "UNK_TKN_IDX = 0\n",
        "PAD_TKN_IDX = 1\n",
        "SOS_TKN_IDX = 2\n",
        "EOS_TKN_IDX = 3"
      ],
      "metadata": {
        "id": "5CRiP4zLOhOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`build_vocab_from_iterator` 함수를 사용해 vocab set을 생성할 수 있다. [pytorch docs](https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator)\n",
        "\n",
        "이때 생성되는 vocab set은 `toechtext.vocab.Vocab` 객체인데, `get_itos()`, `get_stoi()` 등의 함수로 vocab set의 기능을 제공한다. [pytorch docs](https://pytorch.org/text/stable/vocab.html#vocab)"
      ],
      "metadata": {
        "id": "Ei0EaB8DRsK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary set(어휘집) 생성\n",
        "vocab_transform = {} # integer encoding을 수행하는 vocab set의 묶음\n",
        "\n",
        "# (helper function) 데이터셋에서 특정 language의 것만 반환하는 generator function.\n",
        "# `yield` 키워드에 주목하자.\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "  language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "  tokenizer = token_transform[language]\n",
        "  \n",
        "  for data_sample in data_iter:\n",
        "    sentence = data_sample[language_index[language]]\n",
        "    yield tokenizer(sentence)\n",
        "\n",
        "# vocab set: SRC LANGUAGE\n",
        "train_iter = Multi30k(root='./data', split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "vocab_transform[SRC_LANGUAGE] \\\n",
        "  = build_vocab_from_iterator(\n",
        "      iterator = yield_tokens(train_iter, SRC_LANGUAGE),\n",
        "      min_freq = 1,\n",
        "      specials = special_symbols,\n",
        "      special_first = True # `specials`의 토큰이 가장 앞의 index를 가지도록 설정\n",
        "    )\n",
        "vocab_transform[SRC_LANGUAGE].set_default_index(UNK_TKN_IDX)\n",
        "\n",
        "# vocab set: TGT LANGUAGE\n",
        "train_iter = Multi30k(root='./data', split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "vocab_transform[TGT_LANGUAGE] \\\n",
        "  = build_vocab_from_iterator(\n",
        "      iterator = yield_tokens(train_iter, TGT_LANGUAGE),\n",
        "      min_freq = 1,\n",
        "      specials = special_symbols,\n",
        "      special_first = True\n",
        "    )\n",
        "vocab_transform[TGT_LANGUAGE].set_default_index(UNK_TKN_IDX)\n",
        "\n",
        "print(vocab_transform[SRC_LANGUAGE].get_default_index())\n",
        "print(vocab_transform[SRC_LANGUAGE](token_transform[SRC_LANGUAGE](\"Eine Gruppe von Menschen steht vor einem Iglu .\")))"
      ],
      "metadata": {
        "id": "lipzICe8PPkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb64b4b-9a3e-4c9c-dfec-b2d2e1b85fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[15, 39, 25, 55, 31, 29, 7, 6133, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델링\n",
        "\n",
        "1. Embedding: token embedding & positional encoding\n",
        "2. Transformer: `Seq2SeqTranformer`\n",
        "3. Output Layer"
      ],
      "metadata": {
        "id": "9Gbz4ILXtI__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "_qsYibK3U353"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "p2draCM6VN2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Embedding"
      ],
      "metadata": {
        "id": "HBt5grIlVQCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size: int, emb_size: int):\n",
        "    super(TokenEmbedding, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "    self.emb_size = emb_size\n",
        "  \n",
        "  def forward(self, tokens: Tensor):\n",
        "    out = self.embedding(tokens.long())\n",
        "    out *= math.sqrt(self.emb_size) # scaling\n",
        "    return out"
      ],
      "metadata": {
        "id": "-_RXZT-sVNhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "\n",
        "$$\n",
        "PE(pos) = \n",
        "\\begin{cases}\n",
        "  \\sin(\\omega_k \\cdot pos) & \\text{if} \\; i = 2k \\\\\n",
        "  \\cos(\\omega_k \\cdot pos) & \\text{if} \\; i = 2k+1 \n",
        "\\end{cases} \n",
        "\\quad \\left( \\omega_k = \\frac{1}{1000^{k/d}} \\right)\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "Note: $\\omega_k = \\frac{1}{1000^{k/d}} = \\exp \\left( \\log \\frac{1}{1000^{k/d}} \\right) = \\exp \\left( - \\log (1000^{k/d}) \\right) = \\exp \\left( - \\log (1000) \\times {k/d} \\right)$"
      ],
      "metadata": {
        "id": "4cu3R1DAtMQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, emb_size:int, maxlen: int = 5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "\n",
        "    # $1000^{k/d}$를 실제로 계산하려고 하면 underflow가 발생할 수 있으니 log trick을 사용\n",
        "    frequency = torch.exp(- math.log(1000) * (torch.arange(0, emb_size, 2) / emb_size))\n",
        "\n",
        "    # sinusoidla encoding by sin & cos\n",
        "    pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "    pos_embedding[:, 0::2] = torch.sin(pos * frequency) # 짝수 인덱스\n",
        "    pos_embedding[:, 1::2] = torch.cos(pos * frequency) # 홀수 인덱스\n",
        "    pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "    self.register_buffer('pos_embedding', pos_embedding) # ref. https://powerofsummary.tistory.com/158\n",
        "\n",
        "  def forward(self, token_embedding: Tensor):\n",
        "    token_length = token_embedding.size(0)\n",
        "    return token_embedding + self.pos_embedding[:token_length, :]"
      ],
      "metadata": {
        "id": "Evq5gzabtFsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2SeqTranformer\n",
        "\n",
        "`nn.Transformer()` 모듈을 사용한다! [pytorch docs](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#transformer)"
      ],
      "metadata": {
        "id": "fzc9VeQbZYl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "  def __init__(self, \n",
        "               num_encoder_layers: int, num_decoder_layers:int, \n",
        "               emb_size: int, nhead:int,\n",
        "               src_vocab_size:int, tgt_vocab_size:int,  \n",
        "               dim_feedforward:int = 512):\n",
        "    super(Seq2SeqTransformer, self).__init__()\n",
        "    \n",
        "    self.src_tkn_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "    self.tgt_tkn_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "    self.positional_encoding = PositionalEncoding(emb_size)\n",
        "\n",
        "    self.transformer = nn.Transformer(\n",
        "        d_model = emb_size,\n",
        "        num_encoder_layers = num_encoder_layers,\n",
        "        num_decoder_layers = num_decoder_layers,\n",
        "        dim_feedforward = dim_feedforward)\n",
        "\n",
        "    self.output_layer = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "  \n",
        "  def forward(self, \n",
        "              src: Tensor, tgt: Tensor,\n",
        "              src_mask: Tensor, tgt_mask: Tensor,\n",
        "              src_pad_mask: Tensor, tgt_pad_mask: Tensor):\n",
        "    # embedding\n",
        "    src_emb = self.positional_encoding(self.src_tkn_emb(src))\n",
        "    tgt_emb = self.positional_encoding(self.tgt_tkn_emb(tgt))\n",
        "\n",
        "    # transformer\n",
        "    outs = self.transformer(\n",
        "        src_emb, tgt_emb, \n",
        "        src_mask, tgt_mask, None, \n",
        "        src_pad_mask, tgt_pad_mask, src_pad_mask\n",
        "      )\n",
        "\n",
        "    # output layer\n",
        "    out = self.output_layer(outs)\n",
        "    return out\n",
        "\n",
        "  def encode(self, src: Tensor, src_mask: Tensor):\n",
        "    src_emb = self.positional_encoding(self.src_tkn_emb(src))\n",
        "    return self.transformer.encoder(src_emb, src_mask)\n",
        "\n",
        "  def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "    tgt_emb = self.positional_encoding(self.tgt_tkn_emb(tgt))\n",
        "    return self.transformer.decoder(tgt_emb, memory, tgt_mask)\n"
      ],
      "metadata": {
        "id": "J-1CGzoGZQKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "gBMSq4xp0SRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 하이퍼 파라미터"
      ],
      "metadata": {
        "id": "BZeTmApm1Zln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "-07qpw7Tku1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 선언"
      ],
      "metadata": {
        "id": "Tbp8RIHn1Xu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "  if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_TKN_IDX)\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "Y_n1fkAI0XME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문자열 → 배치 텐서\n",
        "\n",
        "데이터 반복자(iterator)는 raw한 문자열의 쌍을 생성합니다. 이 문자열 쌍들을 정의한 Transformer에서 처리할 수 있도록 텐서 묶음(batched tensor)으로 변환해야 합니다. "
      ],
      "metadata": {
        "id": "aahALz1S2I9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(raw_batch):\n",
        "  src_batch, tgt_batch = [], []\n",
        "  for src_sample, tgt_sample in raw_batch:\n",
        "    src_sample = src_sample.rstrip(\"\\n\")\n",
        "    src_sample = token_transform[SRC_LANGUAGE](src_sample) # tokenize\n",
        "    src_sample = vocab_transform[SRC_LANGUAGE](src_sample) # integer encoding\n",
        "    src_sample = torch.cat((torch.tensor([SOS_TKN_IDX]),\n",
        "                      torch.tensor(src_sample),\n",
        "                      torch.tensor([EOS_TKN_IDX])))\n",
        "    src_batch.append(src_sample)\n",
        "\n",
        "    tgt_sample = tgt_sample.rstrip(\"\\n\")\n",
        "    tgt_sample = token_transform[TGT_LANGUAGE](tgt_sample) # tokenize\n",
        "    tgt_sample = vocab_transform[TGT_LANGUAGE](tgt_sample) # integer encoding\n",
        "    tgt_sample = torch.cat((torch.tensor([SOS_TKN_IDX]),\n",
        "                      torch.tensor(tgt_sample),\n",
        "                      torch.tensor([EOS_TKN_IDX])))\n",
        "    tgt_batch.append(tgt_sample)\n",
        "  \n",
        "  src_batch = pad_sequence(src_batch, padding_value=PAD_TKN_IDX)\n",
        "  tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_TKN_IDX)\n",
        "\n",
        "  return src_batch, tgt_batch\n",
        "\n",
        "train_iter = Multi30k(root='./data', split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "for idx, sample in enumerate(train_dataloader):\n",
        "  if idx >= 1: break\n",
        "  print(idx, sample[0].shape, sample) # (max_seq_len, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67O4XY1w5lBV",
        "outputId": "523b3393-2cbb-4c9d-c2b4-336cdea82928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([27, 128]) (tensor([[ 2,  2,  2,  ...,  2,  2,  2],\n",
            "        [22, 85,  6,  ..., 22, 15, 15],\n",
            "        [86, 32, 70,  ..., 47, 39, 18],\n",
            "        ...,\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1]]), tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
            "        [ 20, 166,   7,  ...,  20,   7,   7],\n",
            "        [ 26,  37,  62,  ...,  53,  40,  17],\n",
            "        ...,\n",
            "        [  1,   1,   1,  ...,   1,   1,   1],\n",
            "        [  1,   1,   1,  ...,   1,   1,   1],\n",
            "        [  1,   1,   1,  ...,   1,   1,   1]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask 생성"
      ],
      "metadata": {
        "id": "AZ01r08nGsl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subsequent mask도 구현\n",
        "def generate_square_subsequent_mask(size: int):\n",
        "  mask = torch.triu(torch.ones((size, size), device=DEVICE) == 1) # upper triangular\n",
        "  mask = mask.transpose(0, 1)\n",
        "  mask = mask.float().masked_fill(mask == 0, float('-inf'))\n",
        "  mask = mask.float().masked_fill(mask == 1, float(0.0))\n",
        "  return mask\n",
        "\n",
        "def create_mask(src: Tensor, tgt: Tensor):\n",
        "  src_seq_len = src.shape[0]\n",
        "  tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "  src_pad_mask = (src == PAD_TKN_IDX).transpose(0, 1) # transpose(0, 1): make batch dim first\n",
        "  tgt_pad_mask = (tgt == PAD_TKN_IDX).transpose(0, 1) # (batch_size, max_seq_len)\n",
        "\n",
        "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool) # (max_seq_len, max_seq_len)\n",
        "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "\n",
        "  return src_mask, tgt_mask, src_pad_mask, tgt_pad_mask\n",
        "\n",
        "for idx, sample in enumerate(train_dataloader):\n",
        "  if idx >= 1: break\n",
        "  print(idx, sample[0].shape) # (max_seq_len, batch_size)\n",
        "  src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(sample[0], sample[1])\n",
        "  print(src_mask.shape, tgt_mask.shape) # (max_seq_len, max_seq_len)\n",
        "  print(src_pad_mask.shape, tgt_pad_mask.shape) # (batch_size, max_seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw4Pn2wTGsNV",
        "outputId": "25ca5fa1-39d8-45b4-9377-d770962ad21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([33, 128])\n",
            "torch.Size([33, 33]) torch.Size([36, 36])\n",
            "torch.Size([128, 33]) torch.Size([128, 36])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Function"
      ],
      "metadata": {
        "id": "l4A7Bai9GGG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data load\n",
        "def train_epoch(model, optimizer):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  \n",
        "  train_iter = Multi30k(root='./data', split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "  train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "  for src, tgt in train_dataloader:\n",
        "    src = src.to(DEVICE) # (max_seq_len, batch_size)\n",
        "    tgt = tgt.to(DEVICE)\n",
        "\n",
        "    tgt_input = tgt[:-1, :] # remove <eos> token?\n",
        "    src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "    logits = model(src, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    tgt_out = tgt[1:, :]\n",
        "    loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "tic = time.time()\n",
        "epoch_loss = train_epoch(transformer, optimizer)\n",
        "toc = time.time()\n",
        "print(f'time: {toc - tic:5.1f} sec | train loss: {epoch_loss:8.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmTd3EiMLlbt",
        "outputId": "5e5a680d-cef8-4f0a-a4a8-de5c90b3d247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time:  36.1 sec | train loss:   5.2571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval Function"
      ],
      "metadata": {
        "id": "g2I4EpL2zLnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  val_iter = Multi30k(root='./data', split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "  val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "  for src, tgt in val_dataloader:\n",
        "    src = src.to(DEVICE)\n",
        "    tgt = tgt.to(DEVICE)\n",
        "\n",
        "    tgt_input = tgt[:-1, :]\n",
        "    src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "    logits = model(src, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
        "\n",
        "    tgt_out = tgt[1:, :]\n",
        "    loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(val_dataloader)\n",
        "\n",
        "\n",
        "tic = time.time()\n",
        "val_loss = evaluate(transformer)\n",
        "toc = time.time()\n",
        "print(f'time: {toc - tic:5.1f} sec | val loss: {val_loss:8.4f}')"
      ],
      "metadata": {
        "id": "HaoevgsFYMeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846a7c13-991b-4357-c2c9-65595fd20b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46.3k/46.3k [00:00<00:00, 969kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time:   0.8 sec | val loss:   3.9802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epoch Train"
      ],
      "metadata": {
        "id": "CPDxSITs1g-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 15\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "  tic = time.time()\n",
        "  train_loss = train_epoch(transformer, optimizer)\n",
        "  val_loss = evaluate(transformer)\n",
        "  toc = time.time()\n",
        "  print(f'| epoch: {epoch:3d} | time: {toc - tic:5.1f} sec | train loss: {train_loss:6.4f} | val loss: {val_loss:6.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTNwDv0szuvN",
        "outputId": "6b6f3ebc-2c04-48a6-b8bd-a5b24d4b0b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch:   1 | time:  36.8 sec | train loss: 3.5973 | val loss: 3.1816\n",
            "| epoch:   2 | time:  37.3 sec | train loss: 2.9523 | val loss: 2.7729\n",
            "| epoch:   3 | time:  37.7 sec | train loss: 2.5324 | val loss: 2.5115\n",
            "| epoch:   4 | time:  38.0 sec | train loss: 2.2249 | val loss: 2.3437\n",
            "| epoch:   5 | time:  38.3 sec | train loss: 1.9785 | val loss: 2.2367\n",
            "| epoch:   6 | time:  38.5 sec | train loss: 1.7801 | val loss: 2.1441\n",
            "| epoch:   7 | time:  38.6 sec | train loss: 1.6109 | val loss: 2.0773\n",
            "| epoch:   8 | time:  38.7 sec | train loss: 1.4680 | val loss: 2.0306\n",
            "| epoch:   9 | time:  38.8 sec | train loss: 1.3422 | val loss: 2.0358\n",
            "| epoch:  10 | time:  38.8 sec | train loss: 1.2319 | val loss: 2.0368\n",
            "| epoch:  11 | time:  38.8 sec | train loss: 1.1262 | val loss: 2.0542\n",
            "| epoch:  12 | time:  38.8 sec | train loss: 1.0322 | val loss: 2.0560\n",
            "| epoch:  13 | time:  38.9 sec | train loss: 0.9517 | val loss: 2.0292\n",
            "| epoch:  14 | time:  38.9 sec | train loss: 0.8785 | val loss: 2.0077\n",
            "| epoch:  15 | time:  39.5 sec | train loss: 0.8028 | val loss: 2.0435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 성능 확인"
      ],
      "metadata": {
        "id": "02Q_KLfT2EWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 순차적인 작업들을 하나로 묶는 헬퍼 함수\n",
        "def sequential_transforms(*transforms):\n",
        "  def callback(txt_input):\n",
        "    for transform in transforms:\n",
        "      txt_input = transform(txt_input)\n",
        "    return txt_input\n",
        "  return callback\n",
        "\n",
        "\n",
        "# BOS/EOS를 추가하고 입력 순서(sequence) 인덱스에 대한 텐서를 생성하는 함수\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "  return torch.cat((torch.tensor([SOS_TKN_IDX]),\n",
        "                    torch.tensor(token_ids),\n",
        "                    torch.tensor([EOS_TKN_IDX])))\n",
        "\n",
        "# 출발어(src)와 도착어(tgt) 원시 문자열들을 텐서 인덱스로 변환하는 변형(transform)\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  text_transform[ln] = sequential_transforms(token_transform[ln], # 토큰화(Tokenization)\n",
        "                                               vocab_transform[ln], # 수치화(Numericalization)\n",
        "                                               tensor_transform) # BOS/EOS를 추가하고 텐서를 생성"
      ],
      "metadata": {
        "id": "gmChygqy7EeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src: Tensor, src_mask: Tensor, max_len:int, start_symbol: int = SOS_TKN_IDX):\n",
        "  src = src.to(DEVICE)\n",
        "  src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "  memory = model.encode(src, src_mask) # context vector\n",
        "  ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "  for i in range(max_len - 1):\n",
        "    memory = memory.to(DEVICE)\n",
        "    tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
        "\n",
        "    out = model.decode(ys, memory, tgt_mask)\n",
        "    out = out.transpose(0, 1)\n",
        "\n",
        "    prob = model.output_layer(out[:, -1])\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    next_word = next_word.item()\n",
        "\n",
        "    ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "\n",
        "    if next_word == EOS_TKN_IDX:\n",
        "      break\n",
        "\n",
        "  return ys\n",
        "\n",
        "\n",
        "def translate(model, src_sentence: str):\n",
        "  model.eval()\n",
        "  \n",
        "  src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "  num_tokens = src.shape[0]\n",
        "\n",
        "  src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "  tgt_tokens = greedy_decode(model, src, src_mask, max_len = num_tokens + 5).flatten()\n",
        "  tgt_tokens = list(tgt_tokens.cpu().numpy())\n",
        "\n",
        "  return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(tgt_tokens)).replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FpmrAA817_I",
        "outputId": "251c8d37-4250-472f-a217-d19a84482e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people stand in front of an igloo . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "val_iter = Multi30k(root='./data', split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "for idx, sample in enumerate(val_iter):\n",
        "  if idx >= 5: break\n",
        "  src_sentence = sample[0]\n",
        "  gt_sentence = sample[1]\n",
        "  output_sentence = translate(transformer, src_sentence)\n",
        "\n",
        "  print(f'dutch:         {src_sentence}')\n",
        "  print(f'english(gt):   {gt_sentence}')\n",
        "  print(f'english(pred): {output_sentence}')\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atwrq0m32R47",
        "outputId": "56d73225-ca68-411b-9806-e725e966ffc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dutch: Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
            "\n",
            "english(gt): A group of men are loading cotton onto a truck\n",
            "\n",
            "english(pred):  A group of men are loading into a truck of traffic . \n",
            "--------------------------------------------------\n",
            "dutch: Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
            "\n",
            "english(gt): A man sleeping in a green room on a couch.\n",
            "\n",
            "english(pred):  A man is sleeping on a couch in a green room . \n",
            "--------------------------------------------------\n",
            "dutch: Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
            "\n",
            "english(gt): A boy wearing headphones sits on a woman's shoulders.\n",
            "\n",
            "english(pred):  A boy wearing headphones sits on his shoulders 's shoulders . \n",
            "--------------------------------------------------\n",
            "dutch: Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\n",
            "\n",
            "english(gt): Two men setting up a blue ice fishing hut on an iced over lake\n",
            "\n",
            "english(pred):  Two men are setting up a blue piece of plastic on a lake . \n",
            "--------------------------------------------------\n",
            "dutch: Ein Mann mit beginnender Glatze, der eine rote Rettungsweste trägt, sitzt in einem kleinen Boot.\n",
            "\n",
            "english(gt): A balding man wearing a red life jacket is sitting in a small boat.\n",
            "\n",
            "english(pred):  A balding man is wearing a red life vest wearing a small life jacket . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x0gRGBhEZsxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}